{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 249 Project GMAERF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This portion runs the Graph Autoencoder on Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import and setups\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gae.model import GVAE\n",
    "from gae.optimizer import loss_function\n",
    "import gae.utils\n",
    "\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "args = {\n",
    "  'dataset': 'cora',\n",
    "  'epochs': 200,\n",
    "  'h1_dim': 16,\n",
    "  'h2_dim': 8,\n",
    "  'lr': 1e-2,\n",
    "  'weight_decay': 5e-4,\n",
    "  # 'weight_decay': 0,\n",
    "  'dropout': 0,\n",
    "  'target': 'feat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"using {args['dataset']} dataset\")\n",
    "\n",
    "# preprocessing\n",
    "adj, features = gae.utils.load_data(args['dataset'])\n",
    "n_nodes, feat_dim = features.shape\n",
    "# print(f\"adj dim: {adj.shape}\")\n",
    "# print(adj)\n",
    "# print(f\"fea dim: {features.shape}\")\n",
    "# print(features)\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape) \n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = gae.utils.mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = gae.utils.preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "if args['target'] == 'adj':\n",
    "    pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "elif args['target'] == 'feat':\n",
    "    pos_weight = torch.Tensor([float(features.shape[0] * features.shape[0] - features.sum()) / features.sum()])\n",
    "    norm = features.shape[0] * features.shape[0] / float((features.shape[0] * features.shape[0] - features.sum()) * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'adj'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6be98d55ce96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m   \u001b[0mrecovered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'adj'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madj_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UCLA\\PGM\\project\\GMAERF\\gae\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'adj'"
     ]
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "model = GVAE(feat_dim, args['h1_dim'], args['h2_dim'], args['dropout'], target=args['target'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "hidden_emb = None\n",
    "for epoch in range(args['epochs']):\n",
    "  t = time.time()\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  recovered, mu, logvar = model(features, adj_norm)\n",
    "  if args['target'] == 'adj':\n",
    "    labels = adj_label\n",
    "  elif args['target'] == 'feat':\n",
    "    labels = features\n",
    "  loss = loss_function(preds=recovered, labels=labels,\n",
    "                       mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                       norm=norm, pos_weight=pos_weight,\n",
    "                       target=args['target'])\n",
    "  loss.backward()\n",
    "  cur_loss = loss.item()\n",
    "  optimizer.step()\n",
    "\n",
    "  hidden_emb = mu.data.numpy()\n",
    "\n",
    "  metric = 'cosine'\n",
    "\n",
    "  if args['target'] == 'adj':\n",
    "    roc_curr, ap_curr = gae.utils.get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "    sim_score = (paired_distances(recovered.detach().numpy(), labels.numpy(), metric=metric)).mean()\n",
    "    preds = torch.gt(torch.sigmoid(recovered), 0.5).int()\n",
    "    labels = labels.int()\n",
    "    acc = torch.mean(torch.eq(preds, labels).float())\n",
    "    tp = torch.nonzero(preds * labels).size(0)\n",
    "    fp = torch.nonzero(preds * (labels - 1)).size(0)\n",
    "    fn = torch.nonzero((preds - 1) * labels).size(0)\n",
    "    tn = torch.nonzero((preds - 1) * (labels - 1)).size(0)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "          f\"val_ap={ap_curr:.5f}\", f\"sim_score={sim_score:.5f}\",\n",
    "          f\"time={(time.time()-t):.5f}\", f\"acc={acc:.5f}\", f\"tp={tp}\", \n",
    "          f\"fp={fp}\", f\"fn={fn}\", f\"tn={tn}\", f\"precision={precision:.5f}\", \n",
    "          f\"recall={recall:.5f}\")\n",
    "  elif args['target'] == 'feat':\n",
    "    sim_score = (paired_distances(recovered.detach().numpy(), labels.numpy(), metric=metric)).mean()\n",
    "    preds = torch.gt(torch.sigmoid(recovered), 0.5).int()\n",
    "    labels = labels.int()\n",
    "    acc = torch.mean(torch.eq(preds, labels).float())\n",
    "    tp = torch.nonzero(preds * labels).size(0)\n",
    "    fp = torch.nonzero(preds * (labels - 1)).size(0)\n",
    "    fn = torch.nonzero((preds - 1) * labels).size(0)\n",
    "    tn = torch.nonzero((preds - 1) * (labels - 1)).size(0)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "          f\"sim_score={sim_score:.5f}\", f\"time={(time.time()-t):.5f}\",\n",
    "          f\"acc={acc:.5f}\", f\"tp={tp}\", f\"fp={fp}\", f\"fn={fn}\", f\"tn={tn}\",\n",
    "          f\"precision={precision:.5f}\", f\"recall={recall:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['31336' '1' '0' ... '1' '1' 'Neural_Networks']\n",
      " ['1061127' '0' '1' ... '1' '1' 'Rule_Learning']\n",
      " ['1106406' '0' '0' ... '1' '1' 'Reinforcement_Learning']\n",
      " ...\n",
      " ['1128978' '0' '0' ... '0' '1' 'Genetic_Algorithms']\n",
      " ['117328' '0' '0' ... '1' '0' 'Case_Based']\n",
      " ['24043' '1' '0' ... '1' '0' 'Neural_Networks']]\n"
     ]
    }
   ],
   "source": [
    "## validate\n",
    "\n",
    "# roc_score, ap_score = gae.utils.get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "# print('Test ROC score: ' + str(roc_score))\n",
    "# print('Test AP score: ' + str(ap_score))\n",
    "\n",
    "papers = np.genfromtxt(f\"data/cora.content\", dtype=np.dtype(str))\n",
    "# print(papers[:,0][:,np.newaxis])\n",
    "\n",
    "# print(hidden_emb)\n",
    "# print(papers[:,0][:,np.newaxis].astype(str))\n",
    "# print(papers[:,-1][:,np.newaxis].astype(str))\n",
    "X_train = hidden_emb\n",
    "hidden_emb = torch.gt(torch.sigmoid(torch.from_numpy(hidden_emb.astype(float))), 0.5).int().numpy()\n",
    "hidden_emb = np.append(papers[:,0][:,np.newaxis].astype(str), hidden_emb.astype(str), axis=1)\n",
    "hidden_emb = np.append(hidden_emb.astype(str), papers[:,-1][:,np.newaxis].astype(str), axis=1)\n",
    "print(hidden_emb)\n",
    "y_train = papers[:,-1][:,np.newaxis].astype(str)\n",
    "\n",
    "np.savetxt('hidden_emb_gvae.content', hidden_emb, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5824, Avg. loss: 0.939189\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 7.46, NNZs: 8, Bias: -3.491682, T: 78532, Avg. loss: 0.899884\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 7.21, NNZs: 8, Bias: -3.255640, T: 81240, Avg. loss: 0.860862\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 7.06, NNZs: 8, Bias: -3.740306, T: 83948, Avg. loss: 0.862829\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 6.80, NNZs: 8, Bias: -3.506113, T: 86656, Avg. loss: 0.844122\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 6.78, NNZs: 8, Bias: -4.172225, T: 89364, Avg. loss: 0.818848\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 6.28, NNZs: 8, Bias: -3.948644, T: 92072, Avg. loss: 0.798653\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 6.15, NNZs: 8, Bias: -3.734033, T: 94780, Avg. loss: 0.782178\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 6.03, NNZs: 8, Bias: -3.734299, T: 97488, Avg. loss: 0.734768\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 5.74, NNZs: 8, Bias: -3.536522, T: 100196, Avg. loss: 0.743588\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 5.94, NNZs: 8, Bias: -2.852566, T: 102904, Avg. loss: 0.731184\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 5.83, NNZs: 8, Bias: -3.328157, T: 105612, Avg. loss: 0.708044\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 5.92, NNZs: 8, Bias: -3.973286, T: 108320, Avg. loss: 0.701091\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 5.44, NNZs: 8, Bias: -2.895496, T: 111028, Avg. loss: 0.688075\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 5.44, NNZs: 8, Bias: -3.601617, T: 113736, Avg. loss: 0.657646\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 5.80, NNZs: 8, Bias: -3.340172, T: 116444, Avg. loss: 0.663314\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 5.50, NNZs: 8, Bias: -3.093260, T: 119152, Avg. loss: 0.660352\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 5.70, NNZs: 8, Bias: -2.844048, T: 121860, Avg. loss: 0.656064\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 5.64, NNZs: 8, Bias: -2.999378, T: 124568, Avg. loss: 0.629767\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 5.33, NNZs: 8, Bias: -2.920207, T: 127276, Avg. loss: 0.603974\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 5.39, NNZs: 8, Bias: -3.076252, T: 129984, Avg. loss: 0.616910\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 5.32, NNZs: 8, Bias: -2.924931, T: 132692, Avg. loss: 0.597672\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 5.37, NNZs: 8, Bias: -2.335333, T: 135400, Avg. loss: 0.602053\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 5.45, NNZs: 8, Bias: -2.408902, T: 138108, Avg. loss: 0.587484\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 5.30, NNZs: 8, Bias: -2.479057, T: 140816, Avg. loss: 0.556619\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 5.26, NNZs: 8, Bias: -2.621092, T: 143524, Avg. loss: 0.557898\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 5.17, NNZs: 8, Bias: -2.896420, T: 146232, Avg. loss: 0.559934\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 4.93, NNZs: 8, Bias: -2.424893, T: 148940, Avg. loss: 0.543613\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 4.88, NNZs: 8, Bias: -2.360422, T: 151648, Avg. loss: 0.539763\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 4.58, NNZs: 8, Bias: -2.362166, T: 154356, Avg. loss: 0.541623\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 4.71, NNZs: 8, Bias: -2.427357, T: 157064, Avg. loss: 0.537315\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 4.84, NNZs: 8, Bias: -2.613630, T: 159772, Avg. loss: 0.527308\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 4.41, NNZs: 8, Bias: -2.184437, T: 162480, Avg. loss: 0.520004\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 4.47, NNZs: 8, Bias: -2.609822, T: 165188, Avg. loss: 0.498304\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 4.49, NNZs: 8, Bias: -2.548069, T: 167896, Avg. loss: 0.515013\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 4.36, NNZs: 8, Bias: -2.435716, T: 170604, Avg. loss: 0.495565\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 4.42, NNZs: 8, Bias: -2.319163, T: 173312, Avg. loss: 0.490866\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 4.39, NNZs: 8, Bias: -2.430779, T: 176020, Avg. loss: 0.481461\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 4.42, NNZs: 8, Bias: -2.261592, T: 178728, Avg. loss: 0.480206\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 4.36, NNZs: 8, Bias: -2.317982, T: 181436, Avg. loss: 0.474425\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 4.38, NNZs: 8, Bias: -2.259282, T: 184144, Avg. loss: 0.464795\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 4.36, NNZs: 8, Bias: -2.688317, T: 186852, Avg. loss: 0.456144\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 4.58, NNZs: 8, Bias: -2.580438, T: 189560, Avg. loss: 0.470090\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 4.33, NNZs: 8, Bias: -2.162771, T: 192268, Avg. loss: 0.476618\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 4.49, NNZs: 8, Bias: -2.264332, T: 194976, Avg. loss: 0.457538\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 4.43, NNZs: 8, Bias: -2.114243, T: 197684, Avg. loss: 0.443636\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 4.35, NNZs: 8, Bias: -2.115058, T: 200392, Avg. loss: 0.439476\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 4.04, NNZs: 8, Bias: -1.870397, T: 203100, Avg. loss: 0.451277\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 4.07, NNZs: 8, Bias: -1.969795, T: 205808, Avg. loss: 0.424408\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 3.94, NNZs: 8, Bias: -1.826875, T: 208516, Avg. loss: 0.440973\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 3.86, NNZs: 8, Bias: -1.922017, T: 211224, Avg. loss: 0.434704\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 3.96, NNZs: 8, Bias: -2.110832, T: 213932, Avg. loss: 0.419178\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 3.91, NNZs: 8, Bias: -2.156862, T: 216640, Avg. loss: 0.416840\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 3.85, NNZs: 8, Bias: -2.247115, T: 219348, Avg. loss: 0.422098\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 3.92, NNZs: 8, Bias: -2.200926, T: 222056, Avg. loss: 0.421870\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 3.83, NNZs: 8, Bias: -2.201230, T: 224764, Avg. loss: 0.414224\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 3.59, NNZs: 8, Bias: -1.939382, T: 227472, Avg. loss: 0.403962\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 3.67, NNZs: 8, Bias: -2.113044, T: 230180, Avg. loss: 0.399410\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 3.67, NNZs: 8, Bias: -2.112206, T: 232888, Avg. loss: 0.388588\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 3.71, NNZs: 8, Bias: -2.240867, T: 235596, Avg. loss: 0.389829\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 3.70, NNZs: 8, Bias: -2.157983, T: 238304, Avg. loss: 0.386724\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 3.77, NNZs: 8, Bias: -2.282076, T: 241012, Avg. loss: 0.386347\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 3.70, NNZs: 8, Bias: -2.117382, T: 243720, Avg. loss: 0.391372\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 3.63, NNZs: 8, Bias: -1.914924, T: 246428, Avg. loss: 0.395644\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 3.53, NNZs: 8, Bias: -1.874868, T: 249136, Avg. loss: 0.395367\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 3.57, NNZs: 8, Bias: -2.033670, T: 251844, Avg. loss: 0.397162\n",
      "Total training time: 0.06 seconds.\n",
      "Convergence after 93 epochs took 0.06 seconds\n",
      "-- Epoch 1\n",
      "Norm: 74.97, NNZs: 8, Bias: -121.341831, T: 2708, Avg. loss: 53.545956\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 52.52, NNZs: 8, Bias: -84.472794, T: 5416, Avg. loss: 23.451448\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 33.73, NNZs: 8, Bias: -55.233328, T: 8124, Avg. loss: 14.459886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.13, NNZs: 8, Bias: -49.644577, T: 10832, Avg. loss: 10.540750\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 27.24, NNZs: 8, Bias: -33.158705, T: 13540, Avg. loss: 8.731837\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 23.21, NNZs: 8, Bias: -20.112877, T: 16248, Avg. loss: 7.369563\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 21.63, NNZs: 8, Bias: -17.855775, T: 18956, Avg. loss: 6.272364\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 19.56, NNZs: 8, Bias: -14.523892, T: 21664, Avg. loss: 5.351623\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 17.65, NNZs: 8, Bias: -10.994132, T: 24372, Avg. loss: 4.451630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 16.76, NNZs: 8, Bias: -8.086966, T: 27080, Avg. loss: 4.221198\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 14.93, NNZs: 8, Bias: -8.419427, T: 29788, Avg. loss: 3.901058\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 14.67, NNZs: 8, Bias: -7.853841, T: 32496, Avg. loss: 3.693132\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 14.67, NNZs: 8, Bias: -8.695086, T: 35204, Avg. loss: 3.402956\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 12.99, NNZs: 8, Bias: -6.468182, T: 37912, Avg. loss: 2.956540\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 14.18, NNZs: 8, Bias: -9.206506, T: 40620, Avg. loss: 2.823599\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 14.13, NNZs: 8, Bias: -6.198268, T: 43328, Avg. loss: 2.876359\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 12.39, NNZs: 8, Bias: -4.474898, T: 46036, Avg. loss: 2.603118\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 11.49, NNZs: 8, Bias: -4.702024, T: 48744, Avg. loss: 2.496053\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 11.24, NNZs: 8, Bias: -4.513582, T: 51452, Avg. loss: 2.392735\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 10.56, NNZs: 8, Bias: -4.505779, T: 54160, Avg. loss: 2.229745\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 9.71, NNZs: 8, Bias: -6.420932, T: 56868, Avg. loss: 2.087526\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 9.11, NNZs: 8, Bias: -4.738692, T: 59576, Avg. loss: 2.101968\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 8.84, NNZs: 8, Bias: -4.091944, T: 62284, Avg. loss: 2.049854\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 8.54, NNZs: 8, Bias: -4.083852, T: 64992, Avg. loss: 1.939640\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 8.80, NNZs: 8, Bias: -4.225757, T: 67700, Avg. loss: 1.819997\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 9.07, NNZs: 8, Bias: -4.350014, T: 70408, Avg. loss: 1.799745\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 8.61, NNZs: 8, Bias: -4.202843, T: 73116, Avg. loss: 1.715224\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 8.10, NNZs: 8, Bias: -4.079700, T: 75824, Avg. loss: 1.756787\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 7.93, NNZs: 8, Bias: -3.712403, T: 78532, Avg. loss: 1.608214\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 8.23, NNZs: 8, Bias: -4.567698, T: 81240, Avg. loss: 1.594417\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 8.22, NNZs: 8, Bias: -3.489570, T: 83948, Avg. loss: 1.572536\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 7.68, NNZs: 8, Bias: -3.145719, T: 86656, Avg. loss: 1.493615\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 7.45, NNZs: 8, Bias: -3.711766, T: 89364, Avg. loss: 1.521145\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 7.52, NNZs: 8, Bias: -3.613961, T: 92072, Avg. loss: 1.471519\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 7.83, NNZs: 8, Bias: -3.716403, T: 94780, Avg. loss: 1.374078\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 6.41, NNZs: 8, Bias: -2.987938, T: 97488, Avg. loss: 1.397287\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 6.53, NNZs: 8, Bias: -2.789550, T: 100196, Avg. loss: 1.341205\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 6.63, NNZs: 8, Bias: -2.600209, T: 102904, Avg. loss: 1.274731\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 6.07, NNZs: 8, Bias: -2.886276, T: 105612, Avg. loss: 1.271942\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 5.82, NNZs: 8, Bias: -3.251936, T: 108320, Avg. loss: 1.224306\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 5.53, NNZs: 8, Bias: -3.335281, T: 111028, Avg. loss: 1.245115\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 5.32, NNZs: 8, Bias: -3.074531, T: 113736, Avg. loss: 1.230058\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 5.18, NNZs: 8, Bias: -2.222065, T: 116444, Avg. loss: 1.200653\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 4.99, NNZs: 8, Bias: -2.806585, T: 119152, Avg. loss: 1.182352\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 4.99, NNZs: 8, Bias: -3.051708, T: 121860, Avg. loss: 1.172100\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 5.27, NNZs: 8, Bias: -3.128623, T: 124568, Avg. loss: 1.128796\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 5.02, NNZs: 8, Bias: -3.045339, T: 127276, Avg. loss: 1.149129\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 4.49, NNZs: 8, Bias: -2.737069, T: 129984, Avg. loss: 1.107520\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 4.95, NNZs: 8, Bias: -3.415977, T: 132692, Avg. loss: 1.093717\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 4.62, NNZs: 8, Bias: -2.967578, T: 135400, Avg. loss: 1.049951\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 4.70, NNZs: 8, Bias: -3.114599, T: 138108, Avg. loss: 1.050090\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 4.78, NNZs: 8, Bias: -2.761385, T: 140816, Avg. loss: 1.061832\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 4.74, NNZs: 8, Bias: -2.482088, T: 143524, Avg. loss: 1.066589\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 4.58, NNZs: 8, Bias: -2.412305, T: 146232, Avg. loss: 1.041358\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 4.85, NNZs: 8, Bias: -2.954377, T: 148940, Avg. loss: 1.014265\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 4.74, NNZs: 8, Bias: -2.622794, T: 151648, Avg. loss: 0.987985\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 4.53, NNZs: 8, Bias: -2.423357, T: 154356, Avg. loss: 0.953932\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 4.52, NNZs: 8, Bias: -2.741586, T: 157064, Avg. loss: 0.972391\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 4.14, NNZs: 8, Bias: -2.490065, T: 159772, Avg. loss: 0.961158\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 4.32, NNZs: 8, Bias: -2.426034, T: 162480, Avg. loss: 0.972039\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 4.55, NNZs: 8, Bias: -2.906602, T: 165188, Avg. loss: 0.950491\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 4.27, NNZs: 8, Bias: -2.545353, T: 167896, Avg. loss: 0.940934\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 4.15, NNZs: 8, Bias: -2.662128, T: 170604, Avg. loss: 0.916459\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 3.98, NNZs: 8, Bias: -2.376644, T: 173312, Avg. loss: 0.891885\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 3.99, NNZs: 8, Bias: -2.438042, T: 176020, Avg. loss: 0.912611\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 4.07, NNZs: 8, Bias: -2.437981, T: 178728, Avg. loss: 0.884335\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 4.18, NNZs: 8, Bias: -2.109725, T: 181436, Avg. loss: 0.880886\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 3.92, NNZs: 8, Bias: -2.222354, T: 184144, Avg. loss: 0.858870\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 3.86, NNZs: 8, Bias: -2.219241, T: 186852, Avg. loss: 0.881972\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 3.69, NNZs: 8, Bias: -2.482040, T: 189560, Avg. loss: 0.845685\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 3.53, NNZs: 8, Bias: -2.325208, T: 192268, Avg. loss: 0.850245\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 3.62, NNZs: 8, Bias: -2.374307, T: 194976, Avg. loss: 0.847695\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 3.70, NNZs: 8, Bias: -2.120987, T: 197684, Avg. loss: 0.839598\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 3.53, NNZs: 8, Bias: -2.269213, T: 200392, Avg. loss: 0.827395\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 3.56, NNZs: 8, Bias: -2.173079, T: 203100, Avg. loss: 0.823105\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 3.68, NNZs: 8, Bias: -2.126194, T: 205808, Avg. loss: 0.786613\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 3.39, NNZs: 8, Bias: -2.174208, T: 208516, Avg. loss: 0.802024\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 3.18, NNZs: 8, Bias: -1.512159, T: 211224, Avg. loss: 0.792066\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 3.24, NNZs: 8, Bias: -1.702069, T: 213932, Avg. loss: 0.781368\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 3.28, NNZs: 8, Bias: -2.117438, T: 216640, Avg. loss: 0.754297\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 3.40, NNZs: 8, Bias: -1.979109, T: 219348, Avg. loss: 0.773683\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 3.07, NNZs: 8, Bias: -1.842972, T: 222056, Avg. loss: 0.772779\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 3.17, NNZs: 8, Bias: -1.977612, T: 224764, Avg. loss: 0.752331\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 3.18, NNZs: 8, Bias: -2.152578, T: 227472, Avg. loss: 0.751277\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 3.30, NNZs: 8, Bias: -2.238198, T: 230180, Avg. loss: 0.746733\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 3.11, NNZs: 8, Bias: -1.807740, T: 232888, Avg. loss: 0.742935\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 3.17, NNZs: 8, Bias: -1.936806, T: 235596, Avg. loss: 0.745174\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 2.98, NNZs: 8, Bias: -2.231016, T: 238304, Avg. loss: 0.726651\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 3.06, NNZs: 8, Bias: -2.147356, T: 241012, Avg. loss: 0.727128\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 2.95, NNZs: 8, Bias: -1.942083, T: 243720, Avg. loss: 0.737468\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 3.15, NNZs: 8, Bias: -1.940932, T: 246428, Avg. loss: 0.700748\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 3.02, NNZs: 8, Bias: -1.901004, T: 249136, Avg. loss: 0.725049\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 2.93, NNZs: 8, Bias: -1.743103, T: 251844, Avg. loss: 0.713338\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 3.04, NNZs: 8, Bias: -1.586671, T: 254552, Avg. loss: 0.707256\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 3.00, NNZs: 8, Bias: -1.587361, T: 257260, Avg. loss: 0.709403\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 2.87, NNZs: 8, Bias: -1.780988, T: 259968, Avg. loss: 0.690413\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 2.67, NNZs: 8, Bias: -1.591763, T: 262676, Avg. loss: 0.696950\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 2.84, NNZs: 8, Bias: -1.818567, T: 265384, Avg. loss: 0.676129\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 2.92, NNZs: 8, Bias: -1.855214, T: 268092, Avg. loss: 0.684878\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 2.86, NNZs: 8, Bias: -1.708227, T: 270800, Avg. loss: 0.658131\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 2.95, NNZs: 8, Bias: -1.928400, T: 273508, Avg. loss: 0.669809\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 2.85, NNZs: 8, Bias: -1.565226, T: 276216, Avg. loss: 0.665399\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 2.92, NNZs: 8, Bias: -1.779962, T: 278924, Avg. loss: 0.663728\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 2.75, NNZs: 8, Bias: -1.533113, T: 281632, Avg. loss: 0.659443\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 2.62, NNZs: 8, Bias: -1.885449, T: 284340, Avg. loss: 0.659056\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 105 epochs took 0.04 seconds\n",
      "[0.30206795 0.12961595 0.12961595 ... 0.15435746 0.12961595 0.12961595]\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "classifier = SGDClassifier(verbose=1, max_iter=1000)\n",
    "labelencoder = LabelEncoder()\n",
    "y_train = labelencoder.fit_transform(y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_train, y_train)\n",
    "print(sum(classifier.predict(X_train) == y_train) / y_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This portion runs the GCN on cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setups\n",
    "\n",
    "# from gcn.models import GCN\n",
    "# import gcn.utils\n",
    "\n",
    "# args = {\n",
    "#   'dataset': 'cora',\n",
    "#   'epochs': 200,\n",
    "#   'hidden_dim': 16,\n",
    "#   'lr': 1e-2,\n",
    "#   'weight_decay': 5e-4,\n",
    "#   'dropout': 0.5\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = gcn.utils.load_data()\n",
    "# n_nodes, feat_dim = features.shape\n",
    "\n",
    "# # Model and optimizer\n",
    "# model = GCN(nfeat=feat_dim,\n",
    "#             nhid=args['hidden_dim'],\n",
    "#             nclass=labels.max().item() + 1,\n",
    "#             dropout=args['dropout'])\n",
    "# optimizer = optim.Adam(model.parameters(),\n",
    "#                        lr=args['lr'],\n",
    "#                        weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "# t_total = time.time()\n",
    "\n",
    "# for epoch in range(args['epochs']):\n",
    "#   t = time.time()\n",
    "#   model.train()\n",
    "#   optimizer.zero_grad()\n",
    "#   output = model(features, adj)\n",
    "#   loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "#   acc_train = gcn.utils.accuracy(output[idx_train], labels[idx_train])\n",
    "#   loss_train.backward()\n",
    "#   optimizer.step()\n",
    "\n",
    "#   loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "#   acc_val = gcn.utils.accuracy(output[idx_val], labels[idx_val])\n",
    "#   print(f'Epoch: {(epoch+1):04d}',\n",
    "#         f'loss_train: {loss_train.item():.4f}',\n",
    "#         f'acc_train: {acc_train.item():.4f}',\n",
    "#         f'loss_val: {loss_val.item():.4f}',\n",
    "#         f'acc_val: {acc_val.item():.4f}',\n",
    "#         f'time: {(time.time() - t):.4f}s')\n",
    "\n",
    "# npemb = model.hidden_emb.detach().numpy()\n",
    "# print(npemb.shape)\n",
    "# np.savetxt('hidden_emb.content', npemb)\n",
    "\n",
    "# print(\"Optimization Finished!\")\n",
    "# print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# model.eval()\n",
    "# output = model(features, adj)\n",
    "# loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "# acc_test = gcn.utils.accuracy(output[idx_test], labels[idx_test])\n",
    "# print(f\"Test set results:\",\n",
    "#       f\"loss= {loss_test.item():.4f}\",\n",
    "#       f\"accuracy= {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
