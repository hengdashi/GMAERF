{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitexperimentalconda1a4cce755a7f41b99986d72a1f4a6248",
   "display_name": "Python 3.8.1 64-bit ('experimental': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS 249 Project GMAERF"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This portion runs the Graph Autoencoder on Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import and setups\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gae.model import GVAE\n",
    "from gae.optimizer import loss_function\n",
    "import gae.utils\n",
    "\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "args = {\n",
    "  'dataset': 'cora',\n",
    "  'epochs': 200,\n",
    "  'h1_dim': 16,\n",
    "  'h2_dim': 8,\n",
    "  'lr': 1e-2,\n",
    "  'weight_decay': 5e-4,\n",
    "  # 'weight_decay': 0,\n",
    "  'dropout': 0,\n",
    "  'target': 'feat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"using {args['dataset']} dataset\")\n",
    "\n",
    "# preprocessing\n",
    "adj, features = gae.utils.load_data(args['dataset'])\n",
    "n_nodes, feat_dim = features.shape\n",
    "# print(f\"adj dim: {adj.shape}\")\n",
    "# print(adj)\n",
    "# print(f\"fea dim: {features.shape}\")\n",
    "# print(features)\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape) \n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = gae.utils.mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = gae.utils.preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "if args['target'] == 'adj':\n",
    "    pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "elif args['target'] == 'feat':\n",
    "    pos_weight = torch.Tensor([float(features.shape[0] * features.shape[0] - features.sum()) / features.sum()])\n",
    "    norm = features.shape[0] * features.shape[0] / float((features.shape[0] * features.shape[0] - features.sum()) * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " train_loss=1.36161 sim_score=0.86486 time=0.34486 acc=0.60340 tp=42495 fp=1532313 fn=6721 tn=2299035 precision=0.02698 recall=0.86344\nEpoch  64: train_loss=1.35965 sim_score=0.86613 time=0.36659 acc=0.60215 tp=42542 fp=1537227 fn=6674 tn=2294121 precision=0.02693 recall=0.86439\nEpoch  65: train_loss=1.35679 sim_score=0.86675 time=0.37055 acc=0.59961 tp=42651 fp=1547170 fn=6565 tn=2284178 precision=0.02683 recall=0.86661\nEpoch  66: train_loss=1.35392 sim_score=0.86689 time=0.35653 acc=0.59656 tp=42780 fp=1559146 fn=6436 tn=2272202 precision=0.02671 recall=0.86923\nEpoch  67: train_loss=1.35140 sim_score=0.86619 time=0.34442 acc=0.59312 tp=42925 fp=1572644 fn=6291 tn=2258704 precision=0.02657 recall=0.87218\nEpoch  68: train_loss=1.34869 sim_score=0.86560 time=0.34992 acc=0.59148 tp=43035 fp=1579122 fn=6181 tn=2252226 precision=0.02653 recall=0.87441\nEpoch  69: train_loss=1.34539 sim_score=0.86527 time=0.35170 acc=0.59155 tp=43108 fp=1578906 fn=6108 tn=2252442 precision=0.02658 recall=0.87589\nEpoch  70: train_loss=1.34251 sim_score=0.86507 time=0.35070 acc=0.59445 tp=43075 fp=1567629 fn=6141 tn=2263719 precision=0.02674 recall=0.87522\nEpoch  71: train_loss=1.33916 sim_score=0.86484 time=0.35355 acc=0.59835 tp=43057 fp=1552457 fn=6159 tn=2278891 precision=0.02699 recall=0.87486\nEpoch  72: train_loss=1.33587 sim_score=0.86496 time=0.34616 acc=0.60215 tp=43032 fp=1537709 fn=6184 tn=2293639 precision=0.02722 recall=0.87435\nEpoch  73: train_loss=1.33087 sim_score=0.86515 time=0.34963 acc=0.60478 tp=43065 fp=1527534 fn=6151 tn=2303814 precision=0.02742 recall=0.87502\nEpoch  74: train_loss=1.32661 sim_score=0.86534 time=0.34966 acc=0.60761 tp=43087 fp=1516570 fn=6129 tn=2314778 precision=0.02763 recall=0.87547\nEpoch  75: train_loss=1.32148 sim_score=0.86549 time=0.34684 acc=0.60868 tp=43144 fp=1512476 fn=6072 tn=2318872 precision=0.02773 recall=0.87663\nEpoch  76: train_loss=1.31591 sim_score=0.86564 time=0.35353 acc=0.60934 tp=43230 fp=1509977 fn=5986 tn=2321371 precision=0.02783 recall=0.87837\nEpoch  77: train_loss=1.30986 sim_score=0.86598 time=0.34932 acc=0.61045 tp=43280 fp=1505729 fn=5936 tn=2325619 precision=0.02794 recall=0.87939\nEpoch  78: train_loss=1.30401 sim_score=0.86658 time=0.34796 acc=0.61126 tp=43336 fp=1502656 fn=5880 tn=2328692 precision=0.02803 recall=0.88053\nEpoch  79: train_loss=1.29886 sim_score=0.86760 time=0.35318 acc=0.61305 tp=43395 fp=1495766 fn=5821 tn=2335582 precision=0.02819 recall=0.88173\nEpoch  80: train_loss=1.29337 sim_score=0.86891 time=0.34923 acc=0.61479 tp=43453 fp=1489061 fn=5763 tn=2342287 precision=0.02835 recall=0.88290\nEpoch  81: train_loss=1.28905 sim_score=0.87089 time=0.34826 acc=0.61548 tp=43525 fp=1486459 fn=5691 tn=2344889 precision=0.02845 recall=0.88437\nEpoch  82: train_loss=1.28505 sim_score=0.87318 time=0.35350 acc=0.61737 tp=43514 fp=1479128 fn=5702 tn=2352220 precision=0.02858 recall=0.88414\nEpoch  83: train_loss=1.28152 sim_score=0.87535 time=0.35123 acc=0.61887 tp=43543 fp=1473312 fn=5673 tn=2358036 precision=0.02871 recall=0.88473\nEpoch  84: train_loss=1.27817 sim_score=0.87770 time=0.34666 acc=0.61946 tp=43546 fp=1471050 fn=5670 tn=2360298 precision=0.02875 recall=0.88479\nEpoch  85: train_loss=1.27539 sim_score=0.87971 time=0.35473 acc=0.61981 tp=43554 fp=1469679 fn=5662 tn=2361669 precision=0.02878 recall=0.88496\nEpoch  86: train_loss=1.27237 sim_score=0.88114 time=0.35242 acc=0.62045 tp=43569 fp=1467232 fn=5647 tn=2364116 precision=0.02884 recall=0.88526\nEpoch  87: train_loss=1.26990 sim_score=0.88230 time=0.35513 acc=0.61974 tp=43628 fp=1470022 fn=5588 tn=2361326 precision=0.02882 recall=0.88646\nEpoch  88: train_loss=1.26737 sim_score=0.88298 time=0.35386 acc=0.61920 tp=43683 fp=1472201 fn=5533 tn=2359147 precision=0.02882 recall=0.88758\nEpoch  89: train_loss=1.26564 sim_score=0.88342 time=0.35332 acc=0.61821 tp=43760 fp=1476100 fn=5456 tn=2355248 precision=0.02879 recall=0.88914\nEpoch  90: train_loss=1.26377 sim_score=0.88369 time=0.35243 acc=0.61820 tp=43812 fp=1476200 fn=5404 tn=2355148 precision=0.02882 recall=0.89020\nEpoch  91: train_loss=1.26104 sim_score=0.88390 time=0.35118 acc=0.61740 tp=43842 fp=1479341 fn=5374 tn=2352007 precision=0.02878 recall=0.89081\nEpoch  92: train_loss=1.25924 sim_score=0.88426 time=0.37432 acc=0.61877 tp=43840 fp=1473996 fn=5376 tn=2357352 precision=0.02888 recall=0.89077\nEpoch  93: train_loss=1.25674 sim_score=0.88469 time=0.38326 acc=0.62002 tp=43899 fp=1469237 fn=5317 tn=2362111 precision=0.02901 recall=0.89197\nEpoch  94: train_loss=1.25464 sim_score=0.88554 time=0.36334 acc=0.61983 tp=43902 fp=1469975 fn=5314 tn=2361373 precision=0.02900 recall=0.89203\nEpoch  95: train_loss=1.25241 sim_score=0.88638 time=0.34738 acc=0.62161 tp=43921 fp=1463054 fn=5295 tn=2368294 precision=0.02915 recall=0.89241\nEpoch  96: train_loss=1.25046 sim_score=0.88721 time=0.35178 acc=0.62361 tp=43881 fp=1455258 fn=5335 tn=2376090 precision=0.02927 recall=0.89160\nEpoch  97: train_loss=1.24781 sim_score=0.88772 time=0.35291 acc=0.62419 tp=43919 fp=1453074 fn=5297 tn=2378274 precision=0.02934 recall=0.89237\nEpoch  98: train_loss=1.24563 sim_score=0.88869 time=0.35290 acc=0.62532 tp=43937 fp=1448693 fn=5279 tn=2382655 precision=0.02944 recall=0.89274\nEpoch  99: train_loss=1.24320 sim_score=0.88924 time=0.36388 acc=0.62688 tp=43927 fp=1442623 fn=5289 tn=2388725 precision=0.02955 recall=0.89253\nEpoch 100: train_loss=1.24041 sim_score=0.88955 time=0.35613 acc=0.62595 tp=44020 fp=1446335 fn=5196 tn=2385013 precision=0.02954 recall=0.89442\nEpoch 101: train_loss=1.23791 sim_score=0.88979 time=0.35069 acc=0.62603 tp=44037 fp=1446019 fn=5179 tn=2385329 precision=0.02955 recall=0.89477\nEpoch 102: train_loss=1.23640 sim_score=0.89019 time=0.35414 acc=0.62827 tp=43994 fp=1437283 fn=5222 tn=2394065 precision=0.02970 recall=0.89390\nEpoch 103: train_loss=1.23404 sim_score=0.89107 time=0.34853 acc=0.62873 tp=43998 fp=1435515 fn=5218 tn=2395833 precision=0.02974 recall=0.89398\nEpoch 104: train_loss=1.23160 sim_score=0.89174 time=0.35213 acc=0.62888 tp=44081 fp=1435013 fn=5135 tn=2396335 precision=0.02980 recall=0.89566\nEpoch 105: train_loss=1.22911 sim_score=0.89192 time=0.35629 acc=0.62893 tp=44132 fp=1434859 fn=5084 tn=2396489 precision=0.02984 recall=0.89670\nEpoch 106: train_loss=1.22659 sim_score=0.89190 time=0.35644 acc=0.62854 tp=44222 fp=1436499 fn=4994 tn=2394849 precision=0.02987 recall=0.89853\nEpoch 107: train_loss=1.22413 sim_score=0.89273 time=0.35543 acc=0.63011 tp=44169 fp=1430341 fn=5047 tn=2401007 precision=0.02996 recall=0.89745\nEpoch 108: train_loss=1.22212 sim_score=0.89397 time=0.35462 acc=0.63270 tp=44163 fp=1420272 fn=5053 tn=2411076 precision=0.03016 recall=0.89733\nEpoch 109: train_loss=1.21956 sim_score=0.89442 time=0.35366 acc=0.63234 tp=44202 fp=1421716 fn=5014 tn=2409632 precision=0.03015 recall=0.89812\nEpoch 110: train_loss=1.21688 sim_score=0.89464 time=0.36124 acc=0.63186 tp=44259 fp=1423625 fn=4957 tn=2407723 precision=0.03015 recall=0.89928\nEpoch 111: train_loss=1.21473 sim_score=0.89528 time=0.35767 acc=0.63308 tp=44218 fp=1418848 fn=4998 tn=2412500 precision=0.03022 recall=0.89845\nEpoch 112: train_loss=1.21260 sim_score=0.89619 time=0.36146 acc=0.63485 tp=44222 fp=1412006 fn=4994 tn=2419342 precision=0.03037 recall=0.89853\nEpoch 113: train_loss=1.21035 sim_score=0.89668 time=0.35783 acc=0.63423 tp=44303 fp=1414477 fn=4913 tn=2416871 precision=0.03037 recall=0.90017\nEpoch 114: train_loss=1.20739 sim_score=0.89691 time=0.35395 acc=0.63294 tp=44404 fp=1419592 fn=4812 tn=2411756 precision=0.03033 recall=0.90223\nEpoch 115: train_loss=1.20565 sim_score=0.89795 time=0.35689 acc=0.63428 tp=44398 fp=1414363 fn=4818 tn=2416985 precision=0.03044 recall=0.90211\nEpoch 116: train_loss=1.20334 sim_score=0.89866 time=0.35777 acc=0.63485 tp=44420 fp=1412192 fn=4796 tn=2419156 precision=0.03050 recall=0.90255\nEpoch 117: train_loss=1.20036 sim_score=0.89926 time=0.36231 acc=0.63615 tp=44437 fp=1407159 fn=4779 tn=2424189 precision=0.03061 recall=0.90290\nEpoch 118: train_loss=1.19878 sim_score=0.89979 time=0.36334 acc=0.63710 tp=44466 fp=1403516 fn=4750 tn=2427832 precision=0.03071 recall=0.90349\nEpoch 119: train_loss=1.19660 sim_score=0.90062 time=0.35243 acc=0.63758 tp=44465 fp=1401654 fn=4751 tn=2429694 precision=0.03075 recall=0.90347\nEpoch 120: train_loss=1.19444 sim_score=0.90103 time=0.35110 acc=0.63779 tp=44525 fp=1400873 fn=4691 tn=2430475 precision=0.03080 recall=0.90469\nEpoch 121: train_loss=1.19193 sim_score=0.90102 time=0.35269 acc=0.63767 tp=44597 fp=1401411 fn=4619 tn=2429937 precision=0.03084 recall=0.90615\nEpoch 122: train_loss=1.18988 sim_score=0.90126 time=0.35346 acc=0.63863 tp=44575 fp=1397677 fn=4641 tn=2433671 precision=0.03091 recall=0.90570\nEpoch 123: train_loss=1.18755 sim_score=0.90208 time=0.36375 acc=0.64106 tp=44565 fp=1388220 fn=4651 tn=2443128 precision=0.03110 recall=0.90550\nEpoch 124: train_loss=1.18514 sim_score=0.90228 time=0.35919 acc=0.64094 tp=44619 fp=1388772 fn=4597 tn=2442576 precision=0.03113 recall=0.90660\nEpoch 125: train_loss=1.18298 sim_score=0.90231 time=0.35358 acc=0.64022 tp=44696 fp=1391618 fn=4520 tn=2439730 precision=0.03112 recall=0.90816\nEpoch 126: train_loss=1.18058 sim_score=0.90264 time=0.34904 acc=0.64177 tp=44734 fp=1385654 fn=4482 tn=2445694 precision=0.03127 recall=0.90893\nEpoch 127: train_loss=1.17808 sim_score=0.90344 time=0.35217 acc=0.64348 tp=44692 fp=1378972 fn=4524 tn=2452376 precision=0.03139 recall=0.90808\nEpoch 128: train_loss=1.17633 sim_score=0.90325 time=0.35583 acc=0.64422 tp=44711 fp=1376124 fn=4505 tn=2455224 precision=0.03147 recall=0.90846\nEpoch 129: train_loss=1.17395 sim_score=0.90425 time=0.34857 acc=0.64444 tp=44709 fp=1375255 fn=4507 tn=2456093 precision=0.03149 recall=0.90842\nEpoch 130: train_loss=1.17150 sim_score=0.90381 time=0.35793 acc=0.64389 tp=44781 fp=1377483 fn=4435 tn=2453865 precision=0.03149 recall=0.90989\nEpoch 131: train_loss=1.16917 sim_score=0.90443 time=0.35485 acc=0.64670 tp=44743 fp=1366544 fn=4473 tn=2464804 precision=0.03170 recall=0.90911\nEpoch 132: train_loss=1.16727 sim_score=0.90564 time=0.35723 acc=0.64837 tp=44694 fp=1359987 fn=4522 tn=2471361 precision=0.03182 recall=0.90812\nEpoch 133: train_loss=1.16523 sim_score=0.90550 time=0.35470 acc=0.64742 tp=44807 fp=1363792 fn=4409 tn=2467556 precision=0.03181 recall=0.91042\nEpoch 134: train_loss=1.16364 sim_score=0.90574 time=0.35006 acc=0.64763 tp=44811 fp=1362982 fn=4405 tn=2468366 precision=0.03183 recall=0.91050\nEpoch 135: train_loss=1.16231 sim_score=0.90622 time=0.35893 acc=0.64796 tp=44804 fp=1361714 fn=4412 tn=2469634 precision=0.03185 recall=0.91035\nEpoch 136: train_loss=1.16024 sim_score=0.90593 time=0.35223 acc=0.64909 tp=44820 fp=1357351 fn=4396 tn=2473997 precision=0.03196 recall=0.91068\nEpoch 137: train_loss=1.15766 sim_score=0.90697 time=0.35388 acc=0.65057 tp=44831 fp=1351591 fn=4385 tn=2479757 precision=0.03210 recall=0.91090\nEpoch 138: train_loss=1.15649 sim_score=0.90712 time=0.35441 acc=0.65023 tp=44857 fp=1352928 fn=4359 tn=2478420 precision=0.03209 recall=0.91143\nEpoch 139: train_loss=1.15486 sim_score=0.90708 time=0.35513 acc=0.65098 tp=44883 fp=1350059 fn=4333 tn=2481289 precision=0.03218 recall=0.91196\nEpoch 140: train_loss=1.15330 sim_score=0.90756 time=0.36901 acc=0.65090 tp=44932 fp=1350440 fn=4284 tn=2480908 precision=0.03220 recall=0.91296\nEpoch 141: train_loss=1.15227 sim_score=0.90788 time=0.35855 acc=0.65118 tp=44904 fp=1349323 fn=4312 tn=2482025 precision=0.03221 recall=0.91239\nEpoch 142: train_loss=1.15048 sim_score=0.90815 time=0.36208 acc=0.65243 tp=44918 fp=1344463 fn=4298 tn=2486885 precision=0.03233 recall=0.91267\nEpoch 143: train_loss=1.14991 sim_score=0.90861 time=0.38050 acc=0.65284 tp=44881 fp=1342853 fn=4335 tn=2488495 precision=0.03234 recall=0.91192\nEpoch 144: train_loss=1.14796 sim_score=0.90850 time=0.36726 acc=0.65070 tp=44998 fp=1351266 fn=4218 tn=2480082 precision=0.03223 recall=0.91430\nEpoch 145: train_loss=1.14637 sim_score=0.90850 time=0.37752 acc=0.65245 tp=44942 fp=1344402 fn=4274 tn=2486946 precision=0.03235 recall=0.91316\nEpoch 146: train_loss=1.14512 sim_score=0.90887 time=0.36162 acc=0.65417 tp=44922 fp=1337722 fn=4294 tn=2493626 precision=0.03249 recall=0.91275\nEpoch 147: train_loss=1.14356 sim_score=0.90917 time=0.36348 acc=0.65299 tp=45015 fp=1342379 fn=4201 tn=2488969 precision=0.03245 recall=0.91464\nEpoch 148: train_loss=1.14224 sim_score=0.90905 time=0.34909 acc=0.65208 tp=45039 fp=1345949 fn=4177 tn=2485399 precision=0.03238 recall=0.91513\nEpoch 149: train_loss=1.14070 sim_score=0.90963 time=0.35974 acc=0.65411 tp=45029 fp=1338079 fn=4187 tn=2493269 precision=0.03256 recall=0.91493\nEpoch 150: train_loss=1.13925 sim_score=0.90988 time=0.35774 acc=0.65468 tp=44982 fp=1335800 fn=4234 tn=2495548 precision=0.03258 recall=0.91397\nEpoch 151: train_loss=1.13813 sim_score=0.91005 time=0.36111 acc=0.65450 tp=45053 fp=1336556 fn=4163 tn=2494792 precision=0.03261 recall=0.91541\nEpoch 152: train_loss=1.13653 sim_score=0.91031 time=0.35863 acc=0.65529 tp=45043 fp=1333477 fn=4173 tn=2497871 precision=0.03267 recall=0.91521\nEpoch 153: train_loss=1.13549 sim_score=0.91042 time=0.36984 acc=0.65523 tp=45061 fp=1333733 fn=4155 tn=2497615 precision=0.03268 recall=0.91558\nEpoch 154: train_loss=1.13370 sim_score=0.91059 time=0.37374 acc=0.65565 tp=45110 fp=1332183 fn=4106 tn=2499165 precision=0.03275 recall=0.91657\nEpoch 155: train_loss=1.13250 sim_score=0.91112 time=0.37085 acc=0.65708 tp=45074 fp=1326584 fn=4142 tn=2504764 precision=0.03286 recall=0.91584\nEpoch 156: train_loss=1.13055 sim_score=0.91106 time=0.36545 acc=0.65718 tp=45103 fp=1326224 fn=4113 tn=2505124 precision=0.03289 recall=0.91643\nEpoch 157: train_loss=1.12972 sim_score=0.91126 time=0.37584 acc=0.65678 tp=45114 fp=1327797 fn=4102 tn=2503551 precision=0.03286 recall=0.91665\nEpoch 158: train_loss=1.12765 sim_score=0.91171 time=0.35555 acc=0.65836 tp=45116 fp=1321640 fn=4100 tn=2509708 precision=0.03301 recall=0.91669\nEpoch 159: train_loss=1.12631 sim_score=0.91170 time=0.35285 acc=0.65931 tp=45124 fp=1317969 fn=4092 tn=2513379 precision=0.03310 recall=0.91686\nEpoch 160: train_loss=1.12521 sim_score=0.91181 time=0.35257 acc=0.65941 tp=45168 fp=1317644 fn=4048 tn=2513704 precision=0.03314 recall=0.91775\nEpoch 161: train_loss=1.12340 sim_score=0.91200 time=0.35219 acc=0.65998 tp=45157 fp=1315398 fn=4059 tn=2515950 precision=0.03319 recall=0.91753\nEpoch 162: train_loss=1.12200 sim_score=0.91218 time=0.34823 acc=0.65995 tp=45200 fp=1315557 fn=4016 tn=2515791 precision=0.03322 recall=0.91840\nEpoch 163: train_loss=1.12128 sim_score=0.91260 time=0.35401 acc=0.66058 tp=45183 fp=1313114 fn=4033 tn=2518234 precision=0.03326 recall=0.91806\nEpoch 164: train_loss=1.11928 sim_score=0.91232 time=0.34685 acc=0.66108 tp=45198 fp=1311202 fn=4018 tn=2520146 precision=0.03332 recall=0.91836\nEpoch 165: train_loss=1.11757 sim_score=0.91277 time=0.35302 acc=0.66233 tp=45200 fp=1306324 fn=4016 tn=2525024 precision=0.03344 recall=0.91840\nEpoch 166: train_loss=1.11643 sim_score=0.91266 time=0.35124 acc=0.66193 tp=45281 fp=1307952 fn=3935 tn=2523396 precision=0.03346 recall=0.92005\nEpoch 167: train_loss=1.11488 sim_score=0.91275 time=0.35098 acc=0.66286 tp=45250 fp=1304332 fn=3966 tn=2527016 precision=0.03353 recall=0.91942\nEpoch 168: train_loss=1.11400 sim_score=0.91323 time=0.35610 acc=0.66434 tp=45232 fp=1298567 fn=3984 tn=2532781 precision=0.03366 recall=0.91905\nEpoch 169: train_loss=1.11207 sim_score=0.91327 time=0.35541 acc=0.66326 tp=45276 fp=1302798 fn=3940 tn=2528550 precision=0.03359 recall=0.91994\nEpoch 170: train_loss=1.11034 sim_score=0.91359 time=0.35273 acc=0.66388 tp=45290 fp=1300416 fn=3926 tn=2530932 precision=0.03366 recall=0.92023\nEpoch 171: train_loss=1.10908 sim_score=0.91387 time=0.35611 acc=0.66465 tp=45295 fp=1297414 fn=3921 tn=2533934 precision=0.03373 recall=0.92033\nEpoch 172: train_loss=1.10810 sim_score=0.91411 time=0.35170 acc=0.66440 tp=45311 fp=1298393 fn=3905 tn=2532955 precision=0.03372 recall=0.92066\nEpoch 173: train_loss=1.10643 sim_score=0.91447 time=0.34701 acc=0.66637 tp=45285 fp=1290732 fn=3931 tn=2540616 precision=0.03390 recall=0.92013\nEpoch 174: train_loss=1.10443 sim_score=0.91454 time=0.35235 acc=0.66639 tp=45370 fp=1290736 fn=3846 tn=2540612 precision=0.03396 recall=0.92185\nEpoch 175: train_loss=1.10286 sim_score=0.91449 time=0.35185 acc=0.66618 tp=45432 fp=1291632 fn=3784 tn=2539716 precision=0.03398 recall=0.92311\nEpoch 176: train_loss=1.10212 sim_score=0.91486 time=0.34754 acc=0.66785 tp=45336 fp=1285048 fn=3880 tn=2546300 precision=0.03408 recall=0.92116\nEpoch 177: train_loss=1.10085 sim_score=0.91515 time=0.35453 acc=0.66813 tp=45403 fp=1284019 fn=3813 tn=2547329 precision=0.03415 recall=0.92253\nEpoch 178: train_loss=1.09953 sim_score=0.91509 time=0.35527 acc=0.66724 tp=45415 fp=1287480 fn=3801 tn=2543868 precision=0.03407 recall=0.92277\nEpoch 179: train_loss=1.09890 sim_score=0.91554 time=0.36992 acc=0.66825 tp=45411 fp=1283580 fn=3805 tn=2547768 precision=0.03417 recall=0.92269\nEpoch 180: train_loss=1.09755 sim_score=0.91557 time=0.37001 acc=0.66830 tp=45489 fp=1283472 fn=3727 tn=2547876 precision=0.03423 recall=0.92427\nEpoch 181: train_loss=1.09644 sim_score=0.91572 time=0.36665 acc=0.66825 tp=45495 fp=1283651 fn=3721 tn=2547697 precision=0.03423 recall=0.92439\nEpoch 182: train_loss=1.09548 sim_score=0.91619 time=0.35075 acc=0.66892 tp=45409 fp=1280977 fn=3807 tn=2550371 precision=0.03424 recall=0.92265\nEpoch 183: train_loss=1.09495 sim_score=0.91628 time=0.35628 acc=0.66870 tp=45493 fp=1281897 fn=3723 tn=2549451 precision=0.03427 recall=0.92435\nEpoch 184: train_loss=1.09365 sim_score=0.91651 time=0.35677 acc=0.66926 tp=45470 fp=1279700 fn=3746 tn=2551648 precision=0.03431 recall=0.92389\nEpoch 185: train_loss=1.09222 sim_score=0.91661 time=0.35677 acc=0.66969 tp=45456 fp=1278017 fn=3760 tn=2553331 precision=0.03435 recall=0.92360\nEpoch 186: train_loss=1.09146 sim_score=0.91674 time=0.34896 acc=0.66908 tp=45517 fp=1280449 fn=3699 tn=2550899 precision=0.03433 recall=0.92484\nEpoch 187: train_loss=1.09117 sim_score=0.91704 time=0.35184 acc=0.67010 tp=45508 fp=1276494 fn=3708 tn=2554854 precision=0.03442 recall=0.92466\nEpoch 188: train_loss=1.09012 sim_score=0.91655 time=0.35490 acc=0.66889 tp=45558 fp=1281241 fn=3658 tn=2550107 precision=0.03434 recall=0.92567\nEpoch 189: train_loss=1.08950 sim_score=0.91719 time=0.34920 acc=0.67022 tp=45530 fp=1276046 fn=3686 tn=2555302 precision=0.03445 recall=0.92511\nEpoch 190: train_loss=1.08867 sim_score=0.91751 time=0.35441 acc=0.67087 tp=45545 fp=1273533 fn=3671 tn=2557815 precision=0.03453 recall=0.92541\nEpoch 191: train_loss=1.08756 sim_score=0.91744 time=0.35845 acc=0.67000 tp=45570 fp=1276923 fn=3646 tn=2554425 precision=0.03446 recall=0.92592\nEpoch 192: train_loss=1.08705 sim_score=0.91763 time=0.35243 acc=0.67049 tp=45549 fp=1275017 fn=3667 tn=2556331 precision=0.03449 recall=0.92549\nEpoch 193: train_loss=1.08710 sim_score=0.91785 time=0.35476 acc=0.67085 tp=45529 fp=1273594 fn=3687 tn=2557754 precision=0.03451 recall=0.92509\nEpoch 194: train_loss=1.08614 sim_score=0.91834 time=0.35789 acc=0.67123 tp=45584 fp=1272176 fn=3632 tn=2559172 precision=0.03459 recall=0.92620\nEpoch 195: train_loss=1.08528 sim_score=0.91788 time=0.35709 acc=0.66961 tp=45622 fp=1278487 fn=3594 tn=2552861 precision=0.03445 recall=0.92697\nEpoch 196: train_loss=1.08461 sim_score=0.91789 time=0.35441 acc=0.67085 tp=45582 fp=1273665 fn=3634 tn=2557683 precision=0.03455 recall=0.92616\nEpoch 197: train_loss=1.08452 sim_score=0.91846 time=0.35399 acc=0.67225 tp=45550 fp=1268191 fn=3666 tn=2563157 precision=0.03467 recall=0.92551\nEpoch 198: train_loss=1.08404 sim_score=0.91822 time=0.35928 acc=0.67039 tp=45614 fp=1275452 fn=3602 tn=2555896 precision=0.03453 recall=0.92681\nEpoch 199: train_loss=1.08347 sim_score=0.91872 time=0.35676 acc=0.67130 tp=45593 fp=1271921 fn=3623 tn=2559427 precision=0.03461 recall=0.92639\nEpoch 200: train_loss=1.08256 sim_score=0.91853 time=0.35203 acc=0.67094 tp=45650 fp=1273369 fn=3566 tn=2557979 precision=0.03461 recall=0.92754\n"
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "model = GVAE(feat_dim, args['h1_dim'], args['h2_dim'], args['dropout'], target=args['target'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "hidden_emb = None\n",
    "for epoch in range(args['epochs']):\n",
    "  t = time.time()\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  recovered, mu, logvar = model(features, adj_norm)\n",
    "  if args['target'] == 'adj':\n",
    "    labels = adj_label\n",
    "  elif args['target'] == 'feat':\n",
    "    labels = features\n",
    "  loss = loss_function(preds=recovered, labels=labels,\n",
    "                       mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                       norm=norm, pos_weight=pos_weight,\n",
    "                       target=args['target'])\n",
    "  loss.backward()\n",
    "  cur_loss = loss.item()\n",
    "  optimizer.step()\n",
    "\n",
    "  hidden_emb = mu.data.numpy()\n",
    "\n",
    "  metric = 'cosine'\n",
    "\n",
    "  if args['target'] == 'adj':\n",
    "    roc_curr, ap_curr = gae.utils.get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "    sim_score = (paired_distances(recovered.detach().numpy(), labels.numpy(), metric=metric)).mean()\n",
    "    preds = torch.gt(torch.sigmoid(recovered), 0.5).int()\n",
    "    labels = labels.int()\n",
    "    acc = torch.mean(torch.eq(preds, labels).float())\n",
    "    tp = torch.nonzero(preds * labels).size(0)\n",
    "    fp = torch.nonzero(preds * (labels - 1)).size(0)\n",
    "    fn = torch.nonzero((preds - 1) * labels).size(0)\n",
    "    tn = torch.nonzero((preds - 1) * (labels - 1)).size(0)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "          f\"val_ap={ap_curr:.5f}\", f\"sim_score={sim_score:.5f}\",\n",
    "          f\"time={(time.time()-t):.5f}\", f\"acc={acc:.5f}\", f\"tp={tp}\", \n",
    "          f\"fp={fp}\", f\"fn={fn}\", f\"tn={tn}\", f\"precision={precision:.5f}\", \n",
    "          f\"recall={recall:.5f}\")\n",
    "  elif args['target'] == 'feat':\n",
    "    sim_score = (paired_distances(recovered.detach().numpy(), labels.numpy(), metric=metric)).mean()\n",
    "    preds = torch.gt(torch.sigmoid(recovered), 0.5).int()\n",
    "    labels = labels.int()\n",
    "    acc = torch.mean(torch.eq(preds, labels).float())\n",
    "    tp = torch.nonzero(preds * labels).size(0)\n",
    "    fp = torch.nonzero(preds * (labels - 1)).size(0)\n",
    "    fn = torch.nonzero((preds - 1) * labels).size(0)\n",
    "    tn = torch.nonzero((preds - 1) * (labels - 1)).size(0)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "          f\"sim_score={sim_score:.5f}\", f\"time={(time.time()-t):.5f}\",\n",
    "          f\"acc={acc:.5f}\", f\"tp={tp}\", f\"fp={fp}\", f\"fn={fn}\", f\"tn={tn}\",\n",
    "          f\"precision={precision:.5f}\", f\"recall={recall:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['31336' '1' '0' ... '1' '1' 'Neural_Networks']\n ['1061127' '0' '1' ... '1' '1' 'Rule_Learning']\n ['1106406' '0' '0' ... '1' '1' 'Reinforcement_Learning']\n ...\n ['1128978' '0' '0' ... '0' '1' 'Genetic_Algorithms']\n ['117328' '0' '0' ... '1' '0' 'Case_Based']\n ['24043' '1' '0' ... '1' '0' 'Neural_Networks']]\n"
    }
   ],
   "source": [
    "## validate\n",
    "\n",
    "# roc_score, ap_score = gae.utils.get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "# print('Test ROC score: ' + str(roc_score))\n",
    "# print('Test AP score: ' + str(ap_score))\n",
    "\n",
    "papers = np.genfromtxt(f\"data/cora.content\", dtype=np.dtype(str))\n",
    "# print(papers[:,0][:,np.newaxis])\n",
    "\n",
    "# print(hidden_emb)\n",
    "# print(papers[:,0][:,np.newaxis].astype(str))\n",
    "# print(papers[:,-1][:,np.newaxis].astype(str))\n",
    "X_train = hidden_emb\n",
    "hidden_emb = torch.gt(torch.sigmoid(torch.from_numpy(hidden_emb.astype(float))), 0.5).int().numpy()\n",
    "hidden_emb = np.append(papers[:,0][:,np.newaxis].astype(str), hidden_emb.astype(str), axis=1)\n",
    "hidden_emb = np.append(hidden_emb.astype(str), papers[:,-1][:,np.newaxis].astype(str), axis=1)\n",
    "print(hidden_emb)\n",
    "y_train = papers[:,-1][:,np.newaxis].astype(str)\n",
    "\n",
    "np.savetxt('hidden_emb_gvae.content', hidden_emb, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "5824, Avg. loss: 0.939189\nTotal training time: 0.01 seconds.\n-- Epoch 29\nNorm: 7.46, NNZs: 8, Bias: -3.491682, T: 78532, Avg. loss: 0.899884\nTotal training time: 0.01 seconds.\n-- Epoch 30\nNorm: 7.21, NNZs: 8, Bias: -3.255640, T: 81240, Avg. loss: 0.860862\nTotal training time: 0.01 seconds.\n-- Epoch 31\nNorm: 7.06, NNZs: 8, Bias: -3.740306, T: 83948, Avg. loss: 0.862829\nTotal training time: 0.01 seconds.\n-- Epoch 32\nNorm: 6.80, NNZs: 8, Bias: -3.506113, T: 86656, Avg. loss: 0.844122\nTotal training time: 0.01 seconds.\n-- Epoch 33\nNorm: 6.78, NNZs: 8, Bias: -4.172225, T: 89364, Avg. loss: 0.818848\nTotal training time: 0.01 seconds.\n-- Epoch 34\nNorm: 6.28, NNZs: 8, Bias: -3.948644, T: 92072, Avg. loss: 0.798653\nTotal training time: 0.01 seconds.\n-- Epoch 35\nNorm: 6.15, NNZs: 8, Bias: -3.734033, T: 94780, Avg. loss: 0.782178\nTotal training time: 0.01 seconds.\n-- Epoch 36\nNorm: 6.03, NNZs: 8, Bias: -3.734299, T: 97488, Avg. loss: 0.734768\nTotal training time: 0.01 seconds.\n-- Epoch 37\nNorm: 5.74, NNZs: 8, Bias: -3.536522, T: 100196, Avg. loss: 0.743588\nTotal training time: 0.01 seconds.\n-- Epoch 38\nNorm: 5.94, NNZs: 8, Bias: -2.852566, T: 102904, Avg. loss: 0.731184\nTotal training time: 0.01 seconds.\n-- Epoch 39\nNorm: 5.83, NNZs: 8, Bias: -3.328157, T: 105612, Avg. loss: 0.708044\nTotal training time: 0.01 seconds.\n-- Epoch 40\nNorm: 5.92, NNZs: 8, Bias: -3.973286, T: 108320, Avg. loss: 0.701091\nTotal training time: 0.01 seconds.\n-- Epoch 41\nNorm: 5.44, NNZs: 8, Bias: -2.895496, T: 111028, Avg. loss: 0.688075\nTotal training time: 0.01 seconds.\n-- Epoch 42\nNorm: 5.44, NNZs: 8, Bias: -3.601617, T: 113736, Avg. loss: 0.657646\nTotal training time: 0.01 seconds.\n-- Epoch 43\nNorm: 5.80, NNZs: 8, Bias: -3.340172, T: 116444, Avg. loss: 0.663314\nTotal training time: 0.04 seconds.\n-- Epoch 44\nNorm: 5.50, NNZs: 8, Bias: -3.093260, T: 119152, Avg. loss: 0.660352\nTotal training time: 0.04 seconds.\n-- Epoch 45\nNorm: 5.70, NNZs: 8, Bias: -2.844048, T: 121860, Avg. loss: 0.656064\nTotal training time: 0.04 seconds.\n-- Epoch 46\nNorm: 5.64, NNZs: 8, Bias: -2.999378, T: 124568, Avg. loss: 0.629767\nTotal training time: 0.04 seconds.\n-- Epoch 47\nNorm: 5.33, NNZs: 8, Bias: -2.920207, T: 127276, Avg. loss: 0.603974\nTotal training time: 0.05 seconds.\n-- Epoch 48\nNorm: 5.39, NNZs: 8, Bias: -3.076252, T: 129984, Avg. loss: 0.616910\nTotal training time: 0.05 seconds.\n-- Epoch 49\nNorm: 5.32, NNZs: 8, Bias: -2.924931, T: 132692, Avg. loss: 0.597672\nTotal training time: 0.05 seconds.\n-- Epoch 50\nNorm: 5.37, NNZs: 8, Bias: -2.335333, T: 135400, Avg. loss: 0.602053\nTotal training time: 0.05 seconds.\n-- Epoch 51\nNorm: 5.45, NNZs: 8, Bias: -2.408902, T: 138108, Avg. loss: 0.587484\nTotal training time: 0.05 seconds.\n-- Epoch 52\nNorm: 5.30, NNZs: 8, Bias: -2.479057, T: 140816, Avg. loss: 0.556619\nTotal training time: 0.05 seconds.\n-- Epoch 53\nNorm: 5.26, NNZs: 8, Bias: -2.621092, T: 143524, Avg. loss: 0.557898\nTotal training time: 0.05 seconds.\n-- Epoch 54\nNorm: 5.17, NNZs: 8, Bias: -2.896420, T: 146232, Avg. loss: 0.559934\nTotal training time: 0.05 seconds.\n-- Epoch 55\nNorm: 4.93, NNZs: 8, Bias: -2.424893, T: 148940, Avg. loss: 0.543613\nTotal training time: 0.05 seconds.\n-- Epoch 56\nNorm: 4.88, NNZs: 8, Bias: -2.360422, T: 151648, Avg. loss: 0.539763\nTotal training time: 0.05 seconds.\n-- Epoch 57\nNorm: 4.58, NNZs: 8, Bias: -2.362166, T: 154356, Avg. loss: 0.541623\nTotal training time: 0.05 seconds.\n-- Epoch 58\nNorm: 4.71, NNZs: 8, Bias: -2.427357, T: 157064, Avg. loss: 0.537315\nTotal training time: 0.05 seconds.\n-- Epoch 59\nNorm: 4.84, NNZs: 8, Bias: -2.613630, T: 159772, Avg. loss: 0.527308\nTotal training time: 0.05 seconds.\n-- Epoch 60\nNorm: 4.41, NNZs: 8, Bias: -2.184437, T: 162480, Avg. loss: 0.520004\nTotal training time: 0.05 seconds.\n-- Epoch 61\nNorm: 4.47, NNZs: 8, Bias: -2.609822, T: 165188, Avg. loss: 0.498304\nTotal training time: 0.05 seconds.\n-- Epoch 62\nNorm: 4.49, NNZs: 8, Bias: -2.548069, T: 167896, Avg. loss: 0.515013\nTotal training time: 0.05 seconds.\n-- Epoch 63\nNorm: 4.36, NNZs: 8, Bias: -2.435716, T: 170604, Avg. loss: 0.495565\nTotal training time: 0.05 seconds.\n-- Epoch 64\nNorm: 4.42, NNZs: 8, Bias: -2.319163, T: 173312, Avg. loss: 0.490866\nTotal training time: 0.05 seconds.\n-- Epoch 65\nNorm: 4.39, NNZs: 8, Bias: -2.430779, T: 176020, Avg. loss: 0.481461\nTotal training time: 0.05 seconds.\n-- Epoch 66\nNorm: 4.42, NNZs: 8, Bias: -2.261592, T: 178728, Avg. loss: 0.480206\nTotal training time: 0.05 seconds.\n-- Epoch 67\nNorm: 4.36, NNZs: 8, Bias: -2.317982, T: 181436, Avg. loss: 0.474425\nTotal training time: 0.05 seconds.\n-- Epoch 68\nNorm: 4.38, NNZs: 8, Bias: -2.259282, T: 184144, Avg. loss: 0.464795\nTotal training time: 0.05 seconds.\n-- Epoch 69\nNorm: 4.36, NNZs: 8, Bias: -2.688317, T: 186852, Avg. loss: 0.456144\nTotal training time: 0.05 seconds.\n-- Epoch 70\nNorm: 4.58, NNZs: 8, Bias: -2.580438, T: 189560, Avg. loss: 0.470090\nTotal training time: 0.05 seconds.\n-- Epoch 71\nNorm: 4.33, NNZs: 8, Bias: -2.162771, T: 192268, Avg. loss: 0.476618\nTotal training time: 0.05 seconds.\n-- Epoch 72\nNorm: 4.49, NNZs: 8, Bias: -2.264332, T: 194976, Avg. loss: 0.457538\nTotal training time: 0.05 seconds.\n-- Epoch 73\nNorm: 4.43, NNZs: 8, Bias: -2.114243, T: 197684, Avg. loss: 0.443636\nTotal training time: 0.05 seconds.\n-- Epoch 74\nNorm: 4.35, NNZs: 8, Bias: -2.115058, T: 200392, Avg. loss: 0.439476\nTotal training time: 0.06 seconds.\n-- Epoch 75\nNorm: 4.04, NNZs: 8, Bias: -1.870397, T: 203100, Avg. loss: 0.451277\nTotal training time: 0.06 seconds.\n-- Epoch 76\nNorm: 4.07, NNZs: 8, Bias: -1.969795, T: 205808, Avg. loss: 0.424408\nTotal training time: 0.06 seconds.\n-- Epoch 77\nNorm: 3.94, NNZs: 8, Bias: -1.826875, T: 208516, Avg. loss: 0.440973\nTotal training time: 0.06 seconds.\n-- Epoch 78\nNorm: 3.86, NNZs: 8, Bias: -1.922017, T: 211224, Avg. loss: 0.434704\nTotal training time: 0.06 seconds.\n-- Epoch 79\nNorm: 3.96, NNZs: 8, Bias: -2.110832, T: 213932, Avg. loss: 0.419178\nTotal training time: 0.06 seconds.\n-- Epoch 80\nNorm: 3.91, NNZs: 8, Bias: -2.156862, T: 216640, Avg. loss: 0.416840\nTotal training time: 0.06 seconds.\n-- Epoch 81\nNorm: 3.85, NNZs: 8, Bias: -2.247115, T: 219348, Avg. loss: 0.422098\nTotal training time: 0.06 seconds.\n-- Epoch 82\nNorm: 3.92, NNZs: 8, Bias: -2.200926, T: 222056, Avg. loss: 0.421870\nTotal training time: 0.06 seconds.\n-- Epoch 83\nNorm: 3.83, NNZs: 8, Bias: -2.201230, T: 224764, Avg. loss: 0.414224\nTotal training time: 0.06 seconds.\n-- Epoch 84\nNorm: 3.59, NNZs: 8, Bias: -1.939382, T: 227472, Avg. loss: 0.403962\nTotal training time: 0.06 seconds.\n-- Epoch 85\nNorm: 3.67, NNZs: 8, Bias: -2.113044, T: 230180, Avg. loss: 0.399410\nTotal training time: 0.06 seconds.\n-- Epoch 86\nNorm: 3.67, NNZs: 8, Bias: -2.112206, T: 232888, Avg. loss: 0.388588\nTotal training time: 0.06 seconds.\n-- Epoch 87\nNorm: 3.71, NNZs: 8, Bias: -2.240867, T: 235596, Avg. loss: 0.389829\nTotal training time: 0.06 seconds.\n-- Epoch 88\nNorm: 3.70, NNZs: 8, Bias: -2.157983, T: 238304, Avg. loss: 0.386724\nTotal training time: 0.06 seconds.\n-- Epoch 89\nNorm: 3.77, NNZs: 8, Bias: -2.282076, T: 241012, Avg. loss: 0.386347\nTotal training time: 0.06 seconds.\n-- Epoch 90\nNorm: 3.70, NNZs: 8, Bias: -2.117382, T: 243720, Avg. loss: 0.391372\nTotal training time: 0.06 seconds.\n-- Epoch 91\nNorm: 3.63, NNZs: 8, Bias: -1.914924, T: 246428, Avg. loss: 0.395644\nTotal training time: 0.06 seconds.\n-- Epoch 92\nNorm: 3.53, NNZs: 8, Bias: -1.874868, T: 249136, Avg. loss: 0.395367\nTotal training time: 0.06 seconds.\n-- Epoch 93\nNorm: 3.57, NNZs: 8, Bias: -2.033670, T: 251844, Avg. loss: 0.397162\nTotal training time: 0.06 seconds.\nConvergence after 93 epochs took 0.06 seconds\n-- Epoch 1\nNorm: 74.97, NNZs: 8, Bias: -121.341831, T: 2708, Avg. loss: 53.545956\nTotal training time: 0.00 seconds.\n-- Epoch 2\nNorm: 52.52, NNZs: 8, Bias: -84.472794, T: 5416, Avg. loss: 23.451448\nTotal training time: 0.00 seconds.\n-- Epoch 3\nNorm: 33.73, NNZs: 8, Bias: -55.233328, T: 8124, Avg. loss: 14.459886\nTotal training time: 0.00 seconds.\n-- Epoch 4\nNorm: 32.13, NNZs: 8, Bias: -49.644577, T: 10832, Avg. loss: 10.540750\nTotal training time: 0.00 seconds.\n-- Epoch 5\nNorm: 27.24, NNZs: 8, Bias: -33.158705, T: 13540, Avg. loss: 8.731837\nTotal training time: 0.00 seconds.\n-- Epoch 6\nNorm: 23.21, NNZs: 8, Bias: -20.112877, T: 16248, Avg. loss: 7.369563\nTotal training time: 0.00 seconds.\n-- Epoch 7\nNorm: 21.63, NNZs: 8, Bias: -17.855775, T: 18956, Avg. loss: 6.272364\nTotal training time: 0.00 seconds.\n-- Epoch 8\nNorm: 19.56, NNZs: 8, Bias: -14.523892, T: 21664, Avg. loss: 5.351623\nTotal training time: 0.00 seconds.\n-- Epoch 9\nNorm: 17.65, NNZs: 8, Bias: -10.994132, T: 24372, Avg. loss: 4.451630\nTotal training time: 0.00 seconds.\n-- Epoch 10\nNorm: 16.76, NNZs: 8, Bias: -8.086966, T: 27080, Avg. loss: 4.221198\nTotal training time: 0.00 seconds.\n-- Epoch 11\nNorm: 14.93, NNZs: 8, Bias: -8.419427, T: 29788, Avg. loss: 3.901058\nTotal training time: 0.01 seconds.\n-- Epoch 12\nNorm: 14.67, NNZs: 8, Bias: -7.853841, T: 32496, Avg. loss: 3.693132\nTotal training time: 0.01 seconds.\n-- Epoch 13\nNorm: 14.67, NNZs: 8, Bias: -8.695086, T: 35204, Avg. loss: 3.402956\nTotal training time: 0.01 seconds.\n-- Epoch 14\nNorm: 12.99, NNZs: 8, Bias: -6.468182, T: 37912, Avg. loss: 2.956540\nTotal training time: 0.01 seconds.\n-- Epoch 15\nNorm: 14.18, NNZs: 8, Bias: -9.206506, T: 40620, Avg. loss: 2.823599\nTotal training time: 0.01 seconds.\n-- Epoch 16\nNorm: 14.13, NNZs: 8, Bias: -6.198268, T: 43328, Avg. loss: 2.876359\nTotal training time: 0.01 seconds.\n-- Epoch 17\nNorm: 12.39, NNZs: 8, Bias: -4.474898, T: 46036, Avg. loss: 2.603118\nTotal training time: 0.01 seconds.\n-- Epoch 18\nNorm: 11.49, NNZs: 8, Bias: -4.702024, T: 48744, Avg. loss: 2.496053\nTotal training time: 0.01 seconds.\n-- Epoch 19\nNorm: 11.24, NNZs: 8, Bias: -4.513582, T: 51452, Avg. loss: 2.392735\nTotal training time: 0.01 seconds.\n-- Epoch 20\nNorm: 10.56, NNZs: 8, Bias: -4.505779, T: 54160, Avg. loss: 2.229745\nTotal training time: 0.01 seconds.\n-- Epoch 21\nNorm: 9.71, NNZs: 8, Bias: -6.420932, T: 56868, Avg. loss: 2.087526\nTotal training time: 0.01 seconds.\n-- Epoch 22\nNorm: 9.11, NNZs: 8, Bias: -4.738692, T: 59576, Avg. loss: 2.101968\nTotal training time: 0.01 seconds.\n-- Epoch 23\nNorm: 8.84, NNZs: 8, Bias: -4.091944, T: 62284, Avg. loss: 2.049854\nTotal training time: 0.01 seconds.\n-- Epoch 24\nNorm: 8.54, NNZs: 8, Bias: -4.083852, T: 64992, Avg. loss: 1.939640\nTotal training time: 0.01 seconds.\n-- Epoch 25\nNorm: 8.80, NNZs: 8, Bias: -4.225757, T: 67700, Avg. loss: 1.819997\nTotal training time: 0.01 seconds.\n-- Epoch 26\nNorm: 9.07, NNZs: 8, Bias: -4.350014, T: 70408, Avg. loss: 1.799745\nTotal training time: 0.01 seconds.\n-- Epoch 27\nNorm: 8.61, NNZs: 8, Bias: -4.202843, T: 73116, Avg. loss: 1.715224\nTotal training time: 0.01 seconds.\n-- Epoch 28\nNorm: 8.10, NNZs: 8, Bias: -4.079700, T: 75824, Avg. loss: 1.756787\nTotal training time: 0.01 seconds.\n-- Epoch 29\nNorm: 7.93, NNZs: 8, Bias: -3.712403, T: 78532, Avg. loss: 1.608214\nTotal training time: 0.02 seconds.\n-- Epoch 30\nNorm: 8.23, NNZs: 8, Bias: -4.567698, T: 81240, Avg. loss: 1.594417\nTotal training time: 0.02 seconds.\n-- Epoch 31\nNorm: 8.22, NNZs: 8, Bias: -3.489570, T: 83948, Avg. loss: 1.572536\nTotal training time: 0.02 seconds.\n-- Epoch 32\nNorm: 7.68, NNZs: 8, Bias: -3.145719, T: 86656, Avg. loss: 1.493615\nTotal training time: 0.02 seconds.\n-- Epoch 33\nNorm: 7.45, NNZs: 8, Bias: -3.711766, T: 89364, Avg. loss: 1.521145\nTotal training time: 0.02 seconds.\n-- Epoch 34\nNorm: 7.52, NNZs: 8, Bias: -3.613961, T: 92072, Avg. loss: 1.471519\nTotal training time: 0.02 seconds.\n-- Epoch 35\nNorm: 7.83, NNZs: 8, Bias: -3.716403, T: 94780, Avg. loss: 1.374078\nTotal training time: 0.02 seconds.\n-- Epoch 36\nNorm: 6.41, NNZs: 8, Bias: -2.987938, T: 97488, Avg. loss: 1.397287\nTotal training time: 0.02 seconds.\n-- Epoch 37\nNorm: 6.53, NNZs: 8, Bias: -2.789550, T: 100196, Avg. loss: 1.341205\nTotal training time: 0.02 seconds.\n-- Epoch 38\nNorm: 6.63, NNZs: 8, Bias: -2.600209, T: 102904, Avg. loss: 1.274731\nTotal training time: 0.02 seconds.\n-- Epoch 39\nNorm: 6.07, NNZs: 8, Bias: -2.886276, T: 105612, Avg. loss: 1.271942\nTotal training time: 0.02 seconds.\n-- Epoch 40\nNorm: 5.82, NNZs: 8, Bias: -3.251936, T: 108320, Avg. loss: 1.224306\nTotal training time: 0.02 seconds.\n-- Epoch 41\nNorm: 5.53, NNZs: 8, Bias: -3.335281, T: 111028, Avg. loss: 1.245115\nTotal training time: 0.02 seconds.\n-- Epoch 42\nNorm: 5.32, NNZs: 8, Bias: -3.074531, T: 113736, Avg. loss: 1.230058\nTotal training time: 0.02 seconds.\n-- Epoch 43\nNorm: 5.18, NNZs: 8, Bias: -2.222065, T: 116444, Avg. loss: 1.200653\nTotal training time: 0.02 seconds.\n-- Epoch 44\nNorm: 4.99, NNZs: 8, Bias: -2.806585, T: 119152, Avg. loss: 1.182352\nTotal training time: 0.02 seconds.\n-- Epoch 45\nNorm: 4.99, NNZs: 8, Bias: -3.051708, T: 121860, Avg. loss: 1.172100\nTotal training time: 0.02 seconds.\n-- Epoch 46\nNorm: 5.27, NNZs: 8, Bias: -3.128623, T: 124568, Avg. loss: 1.128796\nTotal training time: 0.02 seconds.\n-- Epoch 47\nNorm: 5.02, NNZs: 8, Bias: -3.045339, T: 127276, Avg. loss: 1.149129\nTotal training time: 0.02 seconds.\n-- Epoch 48\nNorm: 4.49, NNZs: 8, Bias: -2.737069, T: 129984, Avg. loss: 1.107520\nTotal training time: 0.02 seconds.\n-- Epoch 49\nNorm: 4.95, NNZs: 8, Bias: -3.415977, T: 132692, Avg. loss: 1.093717\nTotal training time: 0.02 seconds.\n-- Epoch 50\nNorm: 4.62, NNZs: 8, Bias: -2.967578, T: 135400, Avg. loss: 1.049951\nTotal training time: 0.02 seconds.\n-- Epoch 51\nNorm: 4.70, NNZs: 8, Bias: -3.114599, T: 138108, Avg. loss: 1.050090\nTotal training time: 0.02 seconds.\n-- Epoch 52\nNorm: 4.78, NNZs: 8, Bias: -2.761385, T: 140816, Avg. loss: 1.061832\nTotal training time: 0.02 seconds.\n-- Epoch 53\nNorm: 4.74, NNZs: 8, Bias: -2.482088, T: 143524, Avg. loss: 1.066589\nTotal training time: 0.02 seconds.\n-- Epoch 54\nNorm: 4.58, NNZs: 8, Bias: -2.412305, T: 146232, Avg. loss: 1.041358\nTotal training time: 0.02 seconds.\n-- Epoch 55\nNorm: 4.85, NNZs: 8, Bias: -2.954377, T: 148940, Avg. loss: 1.014265\nTotal training time: 0.02 seconds.\n-- Epoch 56\nNorm: 4.74, NNZs: 8, Bias: -2.622794, T: 151648, Avg. loss: 0.987985\nTotal training time: 0.02 seconds.\n-- Epoch 57\nNorm: 4.53, NNZs: 8, Bias: -2.423357, T: 154356, Avg. loss: 0.953932\nTotal training time: 0.02 seconds.\n-- Epoch 58\nNorm: 4.52, NNZs: 8, Bias: -2.741586, T: 157064, Avg. loss: 0.972391\nTotal training time: 0.02 seconds.\n-- Epoch 59\nNorm: 4.14, NNZs: 8, Bias: -2.490065, T: 159772, Avg. loss: 0.961158\nTotal training time: 0.02 seconds.\n-- Epoch 60\nNorm: 4.32, NNZs: 8, Bias: -2.426034, T: 162480, Avg. loss: 0.972039\nTotal training time: 0.02 seconds.\n-- Epoch 61\nNorm: 4.55, NNZs: 8, Bias: -2.906602, T: 165188, Avg. loss: 0.950491\nTotal training time: 0.02 seconds.\n-- Epoch 62\nNorm: 4.27, NNZs: 8, Bias: -2.545353, T: 167896, Avg. loss: 0.940934\nTotal training time: 0.03 seconds.\n-- Epoch 63\nNorm: 4.15, NNZs: 8, Bias: -2.662128, T: 170604, Avg. loss: 0.916459\nTotal training time: 0.03 seconds.\n-- Epoch 64\nNorm: 3.98, NNZs: 8, Bias: -2.376644, T: 173312, Avg. loss: 0.891885\nTotal training time: 0.03 seconds.\n-- Epoch 65\nNorm: 3.99, NNZs: 8, Bias: -2.438042, T: 176020, Avg. loss: 0.912611\nTotal training time: 0.03 seconds.\n-- Epoch 66\nNorm: 4.07, NNZs: 8, Bias: -2.437981, T: 178728, Avg. loss: 0.884335\nTotal training time: 0.03 seconds.\n-- Epoch 67\nNorm: 4.18, NNZs: 8, Bias: -2.109725, T: 181436, Avg. loss: 0.880886\nTotal training time: 0.03 seconds.\n-- Epoch 68\nNorm: 3.92, NNZs: 8, Bias: -2.222354, T: 184144, Avg. loss: 0.858870\nTotal training time: 0.03 seconds.\n-- Epoch 69\nNorm: 3.86, NNZs: 8, Bias: -2.219241, T: 186852, Avg. loss: 0.881972\nTotal training time: 0.03 seconds.\n-- Epoch 70\nNorm: 3.69, NNZs: 8, Bias: -2.482040, T: 189560, Avg. loss: 0.845685\nTotal training time: 0.03 seconds.\n-- Epoch 71\nNorm: 3.53, NNZs: 8, Bias: -2.325208, T: 192268, Avg. loss: 0.850245\nTotal training time: 0.03 seconds.\n-- Epoch 72\nNorm: 3.62, NNZs: 8, Bias: -2.374307, T: 194976, Avg. loss: 0.847695\nTotal training time: 0.03 seconds.\n-- Epoch 73\nNorm: 3.70, NNZs: 8, Bias: -2.120987, T: 197684, Avg. loss: 0.839598\nTotal training time: 0.03 seconds.\n-- Epoch 74\nNorm: 3.53, NNZs: 8, Bias: -2.269213, T: 200392, Avg. loss: 0.827395\nTotal training time: 0.03 seconds.\n-- Epoch 75\nNorm: 3.56, NNZs: 8, Bias: -2.173079, T: 203100, Avg. loss: 0.823105\nTotal training time: 0.03 seconds.\n-- Epoch 76\nNorm: 3.68, NNZs: 8, Bias: -2.126194, T: 205808, Avg. loss: 0.786613\nTotal training time: 0.03 seconds.\n-- Epoch 77\nNorm: 3.39, NNZs: 8, Bias: -2.174208, T: 208516, Avg. loss: 0.802024\nTotal training time: 0.03 seconds.\n-- Epoch 78\nNorm: 3.18, NNZs: 8, Bias: -1.512159, T: 211224, Avg. loss: 0.792066\nTotal training time: 0.03 seconds.\n-- Epoch 79\nNorm: 3.24, NNZs: 8, Bias: -1.702069, T: 213932, Avg. loss: 0.781368\nTotal training time: 0.03 seconds.\n-- Epoch 80\nNorm: 3.28, NNZs: 8, Bias: -2.117438, T: 216640, Avg. loss: 0.754297\nTotal training time: 0.03 seconds.\n-- Epoch 81\nNorm: 3.40, NNZs: 8, Bias: -1.979109, T: 219348, Avg. loss: 0.773683\nTotal training time: 0.03 seconds.\n-- Epoch 82\nNorm: 3.07, NNZs: 8, Bias: -1.842972, T: 222056, Avg. loss: 0.772779\nTotal training time: 0.03 seconds.\n-- Epoch 83\nNorm: 3.17, NNZs: 8, Bias: -1.977612, T: 224764, Avg. loss: 0.752331\nTotal training time: 0.03 seconds.\n-- Epoch 84\nNorm: 3.18, NNZs: 8, Bias: -2.152578, T: 227472, Avg. loss: 0.751277\nTotal training time: 0.03 seconds.\n-- Epoch 85\nNorm: 3.30, NNZs: 8, Bias: -2.238198, T: 230180, Avg. loss: 0.746733\nTotal training time: 0.03 seconds.\n-- Epoch 86\nNorm: 3.11, NNZs: 8, Bias: -1.807740, T: 232888, Avg. loss: 0.742935\nTotal training time: 0.03 seconds.\n-- Epoch 87\nNorm: 3.17, NNZs: 8, Bias: -1.936806, T: 235596, Avg. loss: 0.745174\nTotal training time: 0.03 seconds.\n-- Epoch 88\nNorm: 2.98, NNZs: 8, Bias: -2.231016, T: 238304, Avg. loss: 0.726651\nTotal training time: 0.03 seconds.\n-- Epoch 89\nNorm: 3.06, NNZs: 8, Bias: -2.147356, T: 241012, Avg. loss: 0.727128\nTotal training time: 0.03 seconds.\n-- Epoch 90\nNorm: 2.95, NNZs: 8, Bias: -1.942083, T: 243720, Avg. loss: 0.737468\nTotal training time: 0.04 seconds.\n-- Epoch 91\nNorm: 3.15, NNZs: 8, Bias: -1.940932, T: 246428, Avg. loss: 0.700748\nTotal training time: 0.04 seconds.\n-- Epoch 92\nNorm: 3.02, NNZs: 8, Bias: -1.901004, T: 249136, Avg. loss: 0.725049\nTotal training time: 0.04 seconds.\n-- Epoch 93\nNorm: 2.93, NNZs: 8, Bias: -1.743103, T: 251844, Avg. loss: 0.713338\nTotal training time: 0.04 seconds.\n-- Epoch 94\nNorm: 3.04, NNZs: 8, Bias: -1.586671, T: 254552, Avg. loss: 0.707256\nTotal training time: 0.04 seconds.\n-- Epoch 95\nNorm: 3.00, NNZs: 8, Bias: -1.587361, T: 257260, Avg. loss: 0.709403\nTotal training time: 0.04 seconds.\n-- Epoch 96\nNorm: 2.87, NNZs: 8, Bias: -1.780988, T: 259968, Avg. loss: 0.690413\nTotal training time: 0.04 seconds.\n-- Epoch 97\nNorm: 2.67, NNZs: 8, Bias: -1.591763, T: 262676, Avg. loss: 0.696950\nTotal training time: 0.04 seconds.\n-- Epoch 98\nNorm: 2.84, NNZs: 8, Bias: -1.818567, T: 265384, Avg. loss: 0.676129\nTotal training time: 0.04 seconds.\n-- Epoch 99\nNorm: 2.92, NNZs: 8, Bias: -1.855214, T: 268092, Avg. loss: 0.684878\nTotal training time: 0.04 seconds.\n-- Epoch 100\nNorm: 2.86, NNZs: 8, Bias: -1.708227, T: 270800, Avg. loss: 0.658131\nTotal training time: 0.04 seconds.\n-- Epoch 101\nNorm: 2.95, NNZs: 8, Bias: -1.928400, T: 273508, Avg. loss: 0.669809\nTotal training time: 0.04 seconds.\n-- Epoch 102\nNorm: 2.85, NNZs: 8, Bias: -1.565226, T: 276216, Avg. loss: 0.665399\nTotal training time: 0.04 seconds.\n-- Epoch 103\nNorm: 2.92, NNZs: 8, Bias: -1.779962, T: 278924, Avg. loss: 0.663728\nTotal training time: 0.04 seconds.\n-- Epoch 104\nNorm: 2.75, NNZs: 8, Bias: -1.533113, T: 281632, Avg. loss: 0.659443\nTotal training time: 0.04 seconds.\n-- Epoch 105\nNorm: 2.62, NNZs: 8, Bias: -1.885449, T: 284340, Avg. loss: 0.659056\nTotal training time: 0.04 seconds.\nConvergence after 105 epochs took 0.04 seconds\n[0.30206795 0.12961595 0.12961595 ... 0.15435746 0.12961595 0.12961595]\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.3s finished\n"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "classifier = SGDClassifier(verbose=1, max_iter=1000)\n",
    "labelencoder = LabelEncoder()\n",
    "y_train = labelencoder.fit_transform(y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.score(X_train, y_train)\n",
    "print(sum(classifier.predict(X_train) == y_train) / y_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This portion runs the GCN on cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setups\n",
    "\n",
    "# from gcn.models import GCN\n",
    "# import gcn.utils\n",
    "\n",
    "# args = {\n",
    "#   'dataset': 'cora',\n",
    "#   'epochs': 200,\n",
    "#   'hidden_dim': 16,\n",
    "#   'lr': 1e-2,\n",
    "#   'weight_decay': 5e-4,\n",
    "#   'dropout': 0.5\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = gcn.utils.load_data()\n",
    "# n_nodes, feat_dim = features.shape\n",
    "\n",
    "# # Model and optimizer\n",
    "# model = GCN(nfeat=feat_dim,\n",
    "#             nhid=args['hidden_dim'],\n",
    "#             nclass=labels.max().item() + 1,\n",
    "#             dropout=args['dropout'])\n",
    "# optimizer = optim.Adam(model.parameters(),\n",
    "#                        lr=args['lr'],\n",
    "#                        weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "# t_total = time.time()\n",
    "\n",
    "# for epoch in range(args['epochs']):\n",
    "#   t = time.time()\n",
    "#   model.train()\n",
    "#   optimizer.zero_grad()\n",
    "#   output = model(features, adj)\n",
    "#   loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "#   acc_train = gcn.utils.accuracy(output[idx_train], labels[idx_train])\n",
    "#   loss_train.backward()\n",
    "#   optimizer.step()\n",
    "\n",
    "#   loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "#   acc_val = gcn.utils.accuracy(output[idx_val], labels[idx_val])\n",
    "#   print(f'Epoch: {(epoch+1):04d}',\n",
    "#         f'loss_train: {loss_train.item():.4f}',\n",
    "#         f'acc_train: {acc_train.item():.4f}',\n",
    "#         f'loss_val: {loss_val.item():.4f}',\n",
    "#         f'acc_val: {acc_val.item():.4f}',\n",
    "#         f'time: {(time.time() - t):.4f}s')\n",
    "\n",
    "# npemb = model.hidden_emb.detach().numpy()\n",
    "# print(npemb.shape)\n",
    "# np.savetxt('hidden_emb.content', npemb)\n",
    "\n",
    "# print(\"Optimization Finished!\")\n",
    "# print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# model.eval()\n",
    "# output = model(features, adj)\n",
    "# loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "# acc_test = gcn.utils.accuracy(output[idx_test], labels[idx_test])\n",
    "# print(f\"Test set results:\",\n",
    "#       f\"loss= {loss_test.item():.4f}\",\n",
    "#       f\"accuracy= {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}