{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitexperimentalconda1a4cce755a7f41b99986d72a1f4a6248",
   "display_name": "Python 3.8.1 64-bit ('experimental': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS 249 Project GMAERF"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This portion runs the Graph Autoencoder on Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import and setups\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gae.model import GVAE\n",
    "from gae.optimizer import loss_function\n",
    "import gae.utils\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, paired_distances\n",
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "args = {\n",
    "  'dataset': 'cora',\n",
    "  'epochs': 200,\n",
    "  'h1_dim': 32,\n",
    "  'h2_dim': 16,\n",
    "  'lr': 1e-2,\n",
    "  'weight_decay': 5e-4,\n",
    "  # 'weight_decay': 0,\n",
    "  'dropout': 0,\n",
    "  'target': 'feat'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"using {args['dataset']} dataset\")\n",
    "\n",
    "# preprocessing\n",
    "adj, features = gae.utils.load_data(args['dataset'])\n",
    "n_nodes, feat_dim = features.shape\n",
    "# print(f\"adj dim: {adj.shape}\")\n",
    "# print(adj)\n",
    "# print(f\"fea dim: {features.shape}\")\n",
    "# print(features)\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = gae.utils.mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = gae.utils.preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "if args['target'] == 'adj':\n",
    "    pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "elif args['target'] == 'feat':\n",
    "    pos_weight = torch.Tensor([float(features.shape[0] * features.shape[0] - features.sum()) / features.sum()])\n",
    "    norm = features.shape[0] * features.shape[0] / float((features.shape[0] * features.shape[0] - features.sum()) * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch   1: train_loss=1.11818 sim_score=0.93681 time=0.19903\nEpoch   2: train_loss=1.07703 sim_score=0.93448 time=0.19431\nEpoch   3: train_loss=1.00149 sim_score=0.93151 time=0.20092\nEpoch   4: train_loss=0.97221 sim_score=0.92713 time=0.19205\nEpoch   5: train_loss=0.96356 sim_score=0.92440 time=0.19379\nEpoch   6: train_loss=0.95985 sim_score=0.92036 time=0.19314\nEpoch   7: train_loss=0.95784 sim_score=0.91679 time=0.19209\nEpoch   8: train_loss=0.95467 sim_score=0.91244 time=0.19103\nEpoch   9: train_loss=0.95131 sim_score=0.90713 time=0.19700\nEpoch  10: train_loss=0.95005 sim_score=0.90399 time=0.19299\nEpoch  11: train_loss=0.94674 sim_score=0.89783 time=0.19453\nEpoch  12: train_loss=0.94418 sim_score=0.89330 time=0.19314\nEpoch  13: train_loss=0.94281 sim_score=0.88918 time=0.19750\nEpoch  14: train_loss=0.94116 sim_score=0.88567 time=0.19113\nEpoch  15: train_loss=0.93808 sim_score=0.87975 time=0.19338\nEpoch  16: train_loss=0.93618 sim_score=0.87658 time=0.19210\nEpoch  17: train_loss=0.93504 sim_score=0.87342 time=0.19178\nEpoch  18: train_loss=0.93267 sim_score=0.86938 time=0.18960\nEpoch  19: train_loss=0.93057 sim_score=0.86658 time=0.19362\nEpoch  20: train_loss=0.93002 sim_score=0.86537 time=0.19198\nEpoch  21: train_loss=0.92740 sim_score=0.86235 time=0.19325\nEpoch  22: train_loss=0.92493 sim_score=0.85943 time=0.19384\nEpoch  23: train_loss=0.92424 sim_score=0.85865 time=0.19233\nEpoch  24: train_loss=0.92316 sim_score=0.85835 time=0.19352\nEpoch  25: train_loss=0.92096 sim_score=0.85670 time=0.19146\nEpoch  26: train_loss=0.91947 sim_score=0.85449 time=0.19459\nEpoch  27: train_loss=0.91781 sim_score=0.85356 time=0.19724\nEpoch  28: train_loss=0.91694 sim_score=0.85336 time=0.18916\nEpoch  29: train_loss=0.91510 sim_score=0.85175 time=0.19124\nEpoch  30: train_loss=0.91376 sim_score=0.85095 time=0.18842\nEpoch  31: train_loss=0.91176 sim_score=0.84884 time=0.19330\nEpoch  32: train_loss=0.91093 sim_score=0.84931 time=0.18883\nEpoch  33: train_loss=0.90990 sim_score=0.84900 time=0.19254\nEpoch  34: train_loss=0.90869 sim_score=0.84775 time=0.18857\nEpoch  35: train_loss=0.90699 sim_score=0.84699 time=0.19148\nEpoch  36: train_loss=0.90521 sim_score=0.84541 time=0.19144\nEpoch  37: train_loss=0.90397 sim_score=0.84486 time=0.18898\nEpoch  38: train_loss=0.90310 sim_score=0.84452 time=0.19207\nEpoch  39: train_loss=0.90110 sim_score=0.84310 time=0.19441\nEpoch  40: train_loss=0.90015 sim_score=0.84295 time=0.18984\nEpoch  41: train_loss=0.89854 sim_score=0.84194 time=0.19274\nEpoch  42: train_loss=0.89816 sim_score=0.84195 time=0.19144\nEpoch  43: train_loss=0.89634 sim_score=0.84056 time=0.19335\nEpoch  44: train_loss=0.89542 sim_score=0.84003 time=0.19135\nEpoch  45: train_loss=0.89467 sim_score=0.84120 time=0.19260\nEpoch  46: train_loss=0.89344 sim_score=0.83933 time=0.18957\nEpoch  47: train_loss=0.89122 sim_score=0.83747 time=0.19025\nEpoch  48: train_loss=0.89130 sim_score=0.83826 time=0.18787\nEpoch  49: train_loss=0.89015 sim_score=0.83757 time=0.19176\nEpoch  50: train_loss=0.88758 sim_score=0.83542 time=0.18892\nEpoch  51: train_loss=0.88693 sim_score=0.83500 time=0.19341\nEpoch  52: train_loss=0.88617 sim_score=0.83493 time=0.18850\nEpoch  53: train_loss=0.88558 sim_score=0.83480 time=0.19242\nEpoch  54: train_loss=0.88330 sim_score=0.83285 time=0.19054\nEpoch  55: train_loss=0.88280 sim_score=0.83246 time=0.19310\nEpoch  56: train_loss=0.88198 sim_score=0.83265 time=0.19025\nEpoch  57: train_loss=0.88036 sim_score=0.83120 time=0.19280\nEpoch  58: train_loss=0.87948 sim_score=0.83085 time=0.18927\nEpoch  59: train_loss=0.87775 sim_score=0.82915 time=0.19115\nEpoch  60: train_loss=0.87703 sim_score=0.82955 time=0.19342\nEpoch  61: train_loss=0.87605 sim_score=0.82900 time=0.18930\nEpoch  62: train_loss=0.87501 sim_score=0.82822 time=0.19234\nEpoch  63: train_loss=0.87402 sim_score=0.82781 time=0.19204\nEpoch  64: train_loss=0.87299 sim_score=0.82706 time=0.19163\nEpoch  65: train_loss=0.87145 sim_score=0.82661 time=0.19380\nEpoch  66: train_loss=0.87023 sim_score=0.82579 time=0.19081\nEpoch  67: train_loss=0.86946 sim_score=0.82529 time=0.19337\nEpoch  68: train_loss=0.86809 sim_score=0.82435 time=0.18939\nEpoch  69: train_loss=0.86722 sim_score=0.82462 time=0.19384\nEpoch  70: train_loss=0.86572 sim_score=0.82328 time=0.19072\nEpoch  71: train_loss=0.86519 sim_score=0.82351 time=0.19286\nEpoch  72: train_loss=0.86402 sim_score=0.82312 time=0.19070\nEpoch  73: train_loss=0.86319 sim_score=0.82226 time=0.19214\nEpoch  74: train_loss=0.86231 sim_score=0.82211 time=0.18983\nEpoch  75: train_loss=0.86092 sim_score=0.82120 time=0.19125\nEpoch  76: train_loss=0.85996 sim_score=0.82064 time=0.19077\nEpoch  77: train_loss=0.85951 sim_score=0.82071 time=0.19078\nEpoch  78: train_loss=0.85751 sim_score=0.81936 time=0.19236\nEpoch  79: train_loss=0.85671 sim_score=0.81903 time=0.19233\nEpoch  80: train_loss=0.85586 sim_score=0.81831 time=0.19078\nEpoch  81: train_loss=0.85473 sim_score=0.81770 time=0.19341\nEpoch  82: train_loss=0.85384 sim_score=0.81762 time=0.19086\nEpoch  83: train_loss=0.85336 sim_score=0.81723 time=0.18749\nEpoch  84: train_loss=0.85162 sim_score=0.81647 time=0.19076\nEpoch  85: train_loss=0.85125 sim_score=0.81626 time=0.18939\nEpoch  86: train_loss=0.85052 sim_score=0.81617 time=0.19133\nEpoch  87: train_loss=0.84936 sim_score=0.81516 time=0.19141\nEpoch  88: train_loss=0.84839 sim_score=0.81481 time=0.19002\nEpoch  89: train_loss=0.84754 sim_score=0.81438 time=0.19309\nEpoch  90: train_loss=0.84694 sim_score=0.81429 time=0.18918\nEpoch  91: train_loss=0.84579 sim_score=0.81348 time=0.19067\nEpoch  92: train_loss=0.84561 sim_score=0.81356 time=0.18827\nEpoch  93: train_loss=0.84480 sim_score=0.81329 time=0.19181\nEpoch  94: train_loss=0.84377 sim_score=0.81272 time=0.19170\nEpoch  95: train_loss=0.84307 sim_score=0.81242 time=0.19118\nEpoch  96: train_loss=0.84259 sim_score=0.81257 time=0.19081\nEpoch  97: train_loss=0.84148 sim_score=0.81171 time=0.19315\nEpoch  98: train_loss=0.84056 sim_score=0.81125 time=0.19074\nEpoch  99: train_loss=0.83995 sim_score=0.81072 time=0.19033\nEpoch 100: train_loss=0.83960 sim_score=0.81073 time=0.19239\nEpoch 101: train_loss=0.83894 sim_score=0.81054 time=0.19245\nEpoch 102: train_loss=0.83822 sim_score=0.81014 time=0.18919\nEpoch 103: train_loss=0.83757 sim_score=0.80990 time=0.19218\nEpoch 104: train_loss=0.83674 sim_score=0.80917 time=0.18985\nEpoch 105: train_loss=0.83637 sim_score=0.80922 time=0.19018\nEpoch 106: train_loss=0.83574 sim_score=0.80879 time=0.19184\nEpoch 107: train_loss=0.83487 sim_score=0.80807 time=0.19206\nEpoch 108: train_loss=0.83480 sim_score=0.80852 time=0.18939\nEpoch 109: train_loss=0.83386 sim_score=0.80768 time=0.19137\nEpoch 110: train_loss=0.83362 sim_score=0.80789 time=0.19027\nEpoch 111: train_loss=0.83266 sim_score=0.80697 time=0.18926\nEpoch 112: train_loss=0.83294 sim_score=0.80756 time=0.19096\nEpoch 113: train_loss=0.83179 sim_score=0.80658 time=0.19125\nEpoch 114: train_loss=0.83168 sim_score=0.80656 time=0.19354\nEpoch 115: train_loss=0.83093 sim_score=0.80622 time=0.19170\nEpoch 116: train_loss=0.83052 sim_score=0.80579 time=0.18898\nEpoch 117: train_loss=0.83018 sim_score=0.80582 time=0.19058\nEpoch 118: train_loss=0.83000 sim_score=0.80582 time=0.18920\nEpoch 119: train_loss=0.82964 sim_score=0.80572 time=0.19078\nEpoch 120: train_loss=0.82874 sim_score=0.80485 time=0.18984\nEpoch 121: train_loss=0.82819 sim_score=0.80461 time=0.19153\nEpoch 122: train_loss=0.82811 sim_score=0.80472 time=0.18906\nEpoch 123: train_loss=0.82741 sim_score=0.80397 time=0.19326\nEpoch 124: train_loss=0.82701 sim_score=0.80390 time=0.18900\nEpoch 125: train_loss=0.82659 sim_score=0.80361 time=0.19254\nEpoch 126: train_loss=0.82643 sim_score=0.80366 time=0.19029\nEpoch 127: train_loss=0.82579 sim_score=0.80306 time=0.19093\nEpoch 128: train_loss=0.82566 sim_score=0.80314 time=0.18959\nEpoch 129: train_loss=0.82523 sim_score=0.80281 time=0.19291\nEpoch 130: train_loss=0.82472 sim_score=0.80257 time=0.18950\nEpoch 131: train_loss=0.82430 sim_score=0.80220 time=0.18938\nEpoch 132: train_loss=0.82362 sim_score=0.80162 time=0.19091\nEpoch 133: train_loss=0.82349 sim_score=0.80165 time=0.19157\nEpoch 134: train_loss=0.82312 sim_score=0.80144 time=0.19035\nEpoch 135: train_loss=0.82293 sim_score=0.80138 time=0.19191\nEpoch 136: train_loss=0.82241 sim_score=0.80096 time=0.18922\nEpoch 137: train_loss=0.82228 sim_score=0.80080 time=0.19281\nEpoch 138: train_loss=0.82185 sim_score=0.80051 time=0.20656\nEpoch 139: train_loss=0.82160 sim_score=0.80033 time=0.19450\nEpoch 140: train_loss=0.82130 sim_score=0.80006 time=0.20104\nEpoch 141: train_loss=0.82113 sim_score=0.80000 time=0.19923\nEpoch 142: train_loss=0.82075 sim_score=0.79964 time=0.19834\nEpoch 143: train_loss=0.82041 sim_score=0.79942 time=0.19873\nEpoch 144: train_loss=0.82023 sim_score=0.79928 time=0.20566\nEpoch 145: train_loss=0.82009 sim_score=0.79910 time=0.20997\nEpoch 146: train_loss=0.81973 sim_score=0.79890 time=0.19837\nEpoch 147: train_loss=0.81936 sim_score=0.79862 time=0.19947\nEpoch 148: train_loss=0.81939 sim_score=0.79864 time=0.19818\nEpoch 149: train_loss=0.81918 sim_score=0.79837 time=0.20066\nEpoch 150: train_loss=0.81880 sim_score=0.79812 time=0.19106\nEpoch 151: train_loss=0.81867 sim_score=0.79801 time=0.19286\nEpoch 152: train_loss=0.81855 sim_score=0.79794 time=0.19988\nEpoch 153: train_loss=0.81821 sim_score=0.79763 time=0.20481\nEpoch 154: train_loss=0.81801 sim_score=0.79748 time=0.20121\nEpoch 155: train_loss=0.81797 sim_score=0.79738 time=0.19970\nEpoch 156: train_loss=0.81771 sim_score=0.79733 time=0.20197\nEpoch 157: train_loss=0.81748 sim_score=0.79700 time=0.19852\nEpoch 158: train_loss=0.81734 sim_score=0.79687 time=0.20217\nEpoch 159: train_loss=0.81718 sim_score=0.79671 time=0.19867\nEpoch 160: train_loss=0.81706 sim_score=0.79654 time=0.20171\nEpoch 161: train_loss=0.81670 sim_score=0.79627 time=0.19866\nEpoch 162: train_loss=0.81662 sim_score=0.79621 time=0.20073\nEpoch 163: train_loss=0.81662 sim_score=0.79626 time=0.19242\nEpoch 164: train_loss=0.81635 sim_score=0.79598 time=0.21171\nEpoch 165: train_loss=0.81614 sim_score=0.79577 time=0.21283\nEpoch 166: train_loss=0.81609 sim_score=0.79569 time=0.19192\nEpoch 167: train_loss=0.81601 sim_score=0.79554 time=0.19584\nEpoch 168: train_loss=0.81583 sim_score=0.79542 time=0.21297\nEpoch 169: train_loss=0.81580 sim_score=0.79540 time=0.20772\nEpoch 170: train_loss=0.81545 sim_score=0.79505 time=0.21054\nEpoch 171: train_loss=0.81530 sim_score=0.79493 time=0.19737\nEpoch 172: train_loss=0.81520 sim_score=0.79488 time=0.19068\nEpoch 173: train_loss=0.81527 sim_score=0.79486 time=0.18913\nEpoch 174: train_loss=0.81496 sim_score=0.79467 time=0.19175\nEpoch 175: train_loss=0.81470 sim_score=0.79435 time=0.19020\nEpoch 176: train_loss=0.81480 sim_score=0.79450 time=0.19415\nEpoch 177: train_loss=0.81463 sim_score=0.79430 time=0.18866\nEpoch 178: train_loss=0.81446 sim_score=0.79412 time=0.19193\nEpoch 179: train_loss=0.81426 sim_score=0.79391 time=0.18712\nEpoch 180: train_loss=0.81417 sim_score=0.79385 time=0.19281\nEpoch 181: train_loss=0.81428 sim_score=0.79390 time=0.19300\nEpoch 182: train_loss=0.81438 sim_score=0.79402 time=0.20479\nEpoch 183: train_loss=0.81404 sim_score=0.79371 time=0.19113\nEpoch 184: train_loss=0.81374 sim_score=0.79346 time=0.20419\nEpoch 185: train_loss=0.81380 sim_score=0.79345 time=0.20564\nEpoch 186: train_loss=0.81390 sim_score=0.79355 time=0.21002\nEpoch 187: train_loss=0.81350 sim_score=0.79318 time=0.20275\nEpoch 188: train_loss=0.81350 sim_score=0.79314 time=0.19313\nEpoch 189: train_loss=0.81352 sim_score=0.79316 time=0.19553\nEpoch 190: train_loss=0.81360 sim_score=0.79318 time=0.19008\nEpoch 191: train_loss=0.81335 sim_score=0.79291 time=0.20426\nEpoch 192: train_loss=0.81318 sim_score=0.79280 time=0.19346\nEpoch 193: train_loss=0.81316 sim_score=0.79278 time=0.19452\nEpoch 194: train_loss=0.81297 sim_score=0.79258 time=0.19044\nEpoch 195: train_loss=0.81287 sim_score=0.79251 time=0.19384\nEpoch 196: train_loss=0.81283 sim_score=0.79244 time=0.20354\nEpoch 197: train_loss=0.81271 sim_score=0.79227 time=0.21073\nEpoch 198: train_loss=0.81263 sim_score=0.79216 time=0.19478\nEpoch 199: train_loss=0.81250 sim_score=0.79203 time=0.20592\nEpoch 200: train_loss=0.81268 sim_score=0.79215 time=0.19381\n"
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "model = GVAE(feat_dim, args['h1_dim'], args['h2_dim'], args['dropout'], target=args['target'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "hidden_emb = None\n",
    "for epoch in range(args['epochs']):\n",
    "  t = time.time()\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  recovered, mu, logvar = model(features, adj_norm)\n",
    "  if args['target'] == 'adj':\n",
    "    labels = adj_label\n",
    "  elif args['target'] == 'feat':\n",
    "    labels = features\n",
    "  loss = loss_function(preds=recovered, labels=labels,\n",
    "                       mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                       norm=norm, pos_weight=pos_weight,\n",
    "                       target=args['target'])\n",
    "  loss.backward()\n",
    "  cur_loss = loss.item()\n",
    "  optimizer.step()\n",
    "\n",
    "  hidden_emb = mu.data.numpy()\n",
    "\n",
    "  metric = 'cosine'\n",
    "\n",
    "  if args['target'] == 'adj':\n",
    "    roc_curr, ap_curr = gae.utils.get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "    sim_score = (paired_distances(recovered.detach().numpy(), labels.numpy(), metric=metric)).mean()\n",
    "    print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "          f\"val_ap={ap_curr:.5f}\", f\"sim_score={sim_score:.5f}\",\n",
    "          f\"time={(time.time()-t):.5f}\")\n",
    "  elif args['target'] == 'feat':\n",
    "    sim_score = (paired_distances(recovered.detach().numpy(), labels.numpy(), metric=metric)).mean()\n",
    "    print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "          f\"sim_score={sim_score:.5f}\", f\"time={(time.time()-t):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test ROC score: 0.5\nTest AP score: 0.5\n"
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f4daa441b464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test AP score: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnpemb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpemb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hidden_emb_gvae.content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpemb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "## validate\n",
    "\n",
    "roc_score, ap_score = gae.utils.get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "print('Test ROC score: ' + str(roc_score))\n",
    "print('Test AP score: ' + str(ap_score))\n",
    "\n",
    "npemb = hidden_emb.detach().numpy()\n",
    "print(npemb.shape)\n",
    "np.savetxt('hidden_emb_gvae.content', npemb)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This portion runs the GCN on cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setups\n",
    "\n",
    "# from gcn.models import GCN\n",
    "# import gcn.utils\n",
    "\n",
    "# args = {\n",
    "#   'dataset': 'cora',\n",
    "#   'epochs': 200,\n",
    "#   'hidden_dim': 16,\n",
    "#   'lr': 1e-2,\n",
    "#   'weight_decay': 5e-4,\n",
    "#   'dropout': 0.5\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = gcn.utils.load_data()\n",
    "# n_nodes, feat_dim = features.shape\n",
    "\n",
    "# # Model and optimizer\n",
    "# model = GCN(nfeat=feat_dim,\n",
    "#             nhid=args['hidden_dim'],\n",
    "#             nclass=labels.max().item() + 1,\n",
    "#             dropout=args['dropout'])\n",
    "# optimizer = optim.Adam(model.parameters(),\n",
    "#                        lr=args['lr'],\n",
    "#                        weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "# t_total = time.time()\n",
    "\n",
    "# for epoch in range(args['epochs']):\n",
    "#   t = time.time()\n",
    "#   model.train()\n",
    "#   optimizer.zero_grad()\n",
    "#   output = model(features, adj)\n",
    "#   loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "#   acc_train = gcn.utils.accuracy(output[idx_train], labels[idx_train])\n",
    "#   loss_train.backward()\n",
    "#   optimizer.step()\n",
    "\n",
    "#   loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "#   acc_val = gcn.utils.accuracy(output[idx_val], labels[idx_val])\n",
    "#   print(f'Epoch: {(epoch+1):04d}',\n",
    "#         f'loss_train: {loss_train.item():.4f}',\n",
    "#         f'acc_train: {acc_train.item():.4f}',\n",
    "#         f'loss_val: {loss_val.item():.4f}',\n",
    "#         f'acc_val: {acc_val.item():.4f}',\n",
    "#         f'time: {(time.time() - t):.4f}s')\n",
    "\n",
    "# npemb = model.hidden_emb.detach().numpy()\n",
    "# print(npemb.shape)\n",
    "# np.savetxt('hidden_emb.content', npemb)\n",
    "\n",
    "# print(\"Optimization Finished!\")\n",
    "# print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# model.eval()\n",
    "# output = model(features, adj)\n",
    "# loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "# acc_test = gcn.utils.accuracy(output[idx_test], labels[idx_test])\n",
    "# print(f\"Test set results:\",\n",
    "#       f\"loss= {loss_test.item():.4f}\",\n",
    "#       f\"accuracy= {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}