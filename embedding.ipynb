{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitexperimentalconda1a4cce755a7f41b99986d72a1f4a6248",
   "display_name": "Python 3.8.1 64-bit ('experimental': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS 249 Project GMAERF"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This portion runs the Graph Autoencoder on Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import and setups\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gae.model import GVAE\n",
    "from gae.optimizer import loss_function\n",
    "import gae.utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "args = {\n",
    "  'dataset': 'cora',\n",
    "  'epochs': 200,\n",
    "  'h1_dim': 32,\n",
    "  'h2_dim': 16,\n",
    "  'lr': 1e-2,\n",
    "  'dropout': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "using cora dataset\nadj dim: (2708, 2708)\nfea dim: torch.Size([2708, 1433])\n"
    }
   ],
   "source": [
    "print(f\"using {args['dataset']} dataset\")\n",
    "\n",
    "## preprocessing\n",
    "adj, features = gae.utils.load_data(args['dataset'])\n",
    "n_nodes, feat_dim = features.shape\n",
    "print(f\"adj dim: {adj.shape}\")\n",
    "print(f\"fea dim: {features.shape}\")\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = gae.utils.mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = gae.utils.preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "# adj_label = sparse_to_tuple(adj_label)\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch   1: train_loss=1.80811 val_ap=0.70124 time=0.33693\nEpoch   2: train_loss=1.62555 val_ap=0.69415 time=0.33081\nEpoch   3: train_loss=1.45576 val_ap=0.68396 time=0.32325\nEpoch   4: train_loss=1.32817 val_ap=0.68144 time=0.32389\nEpoch   5: train_loss=1.24437 val_ap=0.68461 time=0.33732\nEpoch   6: train_loss=1.15958 val_ap=0.68793 time=0.32505\nEpoch   7: train_loss=1.09856 val_ap=0.69191 time=0.31539\nEpoch   8: train_loss=1.00455 val_ap=0.69608 time=0.31305\nEpoch   9: train_loss=0.92773 val_ap=0.70355 time=0.32709\nEpoch  10: train_loss=0.87715 val_ap=0.71287 time=0.35105\nEpoch  11: train_loss=0.81845 val_ap=0.72329 time=0.32789\nEpoch  12: train_loss=0.76881 val_ap=0.73441 time=0.31437\nEpoch  13: train_loss=0.74456 val_ap=0.74372 time=0.31256\nEpoch  14: train_loss=0.71742 val_ap=0.75469 time=0.31542\nEpoch  15: train_loss=0.70370 val_ap=0.76707 time=0.31032\nEpoch  16: train_loss=0.68860 val_ap=0.77752 time=0.31750\nEpoch  17: train_loss=0.67489 val_ap=0.78292 time=0.33153\nEpoch  18: train_loss=0.66648 val_ap=0.78791 time=0.31249\nEpoch  19: train_loss=0.65461 val_ap=0.79481 time=0.33878\nEpoch  20: train_loss=0.63867 val_ap=0.80196 time=0.39856\nEpoch  21: train_loss=0.62651 val_ap=0.80734 time=0.37139\nEpoch  22: train_loss=0.61289 val_ap=0.80597 time=0.36017\nEpoch  23: train_loss=0.59996 val_ap=0.80270 time=0.40691\nEpoch  24: train_loss=0.58834 val_ap=0.79903 time=0.33083\nEpoch  25: train_loss=0.57501 val_ap=0.80136 time=0.37644\nEpoch  26: train_loss=0.56317 val_ap=0.80820 time=0.34741\nEpoch  27: train_loss=0.55644 val_ap=0.81797 time=0.35020\nEpoch  28: train_loss=0.55153 val_ap=0.82539 time=0.31602\nEpoch  29: train_loss=0.55000 val_ap=0.83359 time=0.41058\nEpoch  30: train_loss=0.54665 val_ap=0.84469 time=0.33376\nEpoch  31: train_loss=0.54180 val_ap=0.85847 time=0.33440\nEpoch  32: train_loss=0.53244 val_ap=0.86846 time=0.32749\nEpoch  33: train_loss=0.52598 val_ap=0.87582 time=0.33238\nEpoch  34: train_loss=0.51660 val_ap=0.88279 time=0.33617\nEpoch  35: train_loss=0.51235 val_ap=0.89046 time=0.32442\nEpoch  36: train_loss=0.50830 val_ap=0.89430 time=0.33757\nEpoch  37: train_loss=0.50702 val_ap=0.89507 time=0.32537\nEpoch  38: train_loss=0.50623 val_ap=0.89518 time=0.33165\nEpoch  39: train_loss=0.50471 val_ap=0.89665 time=0.33032\nEpoch  40: train_loss=0.49981 val_ap=0.89896 time=0.32053\nEpoch  41: train_loss=0.49768 val_ap=0.90123 time=0.32050\nEpoch  42: train_loss=0.49467 val_ap=0.90242 time=0.37395\nEpoch  43: train_loss=0.49226 val_ap=0.90254 time=0.34589\nEpoch  44: train_loss=0.48977 val_ap=0.90362 time=0.33163\nEpoch  45: train_loss=0.48695 val_ap=0.90671 time=0.32986\nEpoch  46: train_loss=0.48418 val_ap=0.90802 time=0.30709\nEpoch  47: train_loss=0.48266 val_ap=0.90756 time=0.31198\nEpoch  48: train_loss=0.48176 val_ap=0.90494 time=0.31324\nEpoch  49: train_loss=0.48045 val_ap=0.90329 time=0.31374\nEpoch  50: train_loss=0.47830 val_ap=0.90490 time=0.31273\nEpoch  51: train_loss=0.47798 val_ap=0.90720 time=0.31662\nEpoch  52: train_loss=0.47560 val_ap=0.90751 time=0.31581\nEpoch  53: train_loss=0.47473 val_ap=0.90676 time=0.30909\nEpoch  54: train_loss=0.47343 val_ap=0.90737 time=0.31611\nEpoch  55: train_loss=0.47200 val_ap=0.90779 time=0.33293\nEpoch  56: train_loss=0.47115 val_ap=0.90976 time=0.32389\nEpoch  57: train_loss=0.47099 val_ap=0.91229 time=0.33229\nEpoch  58: train_loss=0.46981 val_ap=0.91397 time=0.33579\nEpoch  59: train_loss=0.46913 val_ap=0.91455 time=0.33173\nEpoch  60: train_loss=0.46870 val_ap=0.91511 time=0.33117\nEpoch  61: train_loss=0.46770 val_ap=0.91622 time=0.52267\nEpoch  62: train_loss=0.46728 val_ap=0.91824 time=0.38593\nEpoch  63: train_loss=0.46659 val_ap=0.92002 time=0.38208\nEpoch  64: train_loss=0.46623 val_ap=0.91921 time=0.37594\nEpoch  65: train_loss=0.46565 val_ap=0.91772 time=0.36744\nEpoch  66: train_loss=0.46512 val_ap=0.91823 time=0.43979\nEpoch  67: train_loss=0.46496 val_ap=0.92130 time=0.40345\nEpoch  68: train_loss=0.46410 val_ap=0.92149 time=0.42168\nEpoch  69: train_loss=0.46359 val_ap=0.91893 time=0.38566\nEpoch  70: train_loss=0.46253 val_ap=0.91744 time=0.38306\nEpoch  71: train_loss=0.46251 val_ap=0.91845 time=0.38053\nEpoch  72: train_loss=0.46161 val_ap=0.91985 time=0.35175\nEpoch  73: train_loss=0.46108 val_ap=0.92007 time=0.31529\nEpoch  74: train_loss=0.46106 val_ap=0.91856 time=0.31992\nEpoch  75: train_loss=0.46042 val_ap=0.91708 time=0.30434\nEpoch  76: train_loss=0.45961 val_ap=0.91874 time=0.30774\nEpoch  77: train_loss=0.45911 val_ap=0.92086 time=0.37098\nEpoch  78: train_loss=0.45847 val_ap=0.92121 time=0.31988\nEpoch  79: train_loss=0.45871 val_ap=0.91980 time=0.38470\nEpoch  80: train_loss=0.45816 val_ap=0.92005 time=0.32564\nEpoch  81: train_loss=0.45738 val_ap=0.92253 time=0.34143\nEpoch  82: train_loss=0.45693 val_ap=0.92274 time=0.31734\nEpoch  83: train_loss=0.45659 val_ap=0.92177 time=0.32403\nEpoch  84: train_loss=0.45639 val_ap=0.92211 time=0.33089\nEpoch  85: train_loss=0.45569 val_ap=0.92321 time=0.31886\nEpoch  86: train_loss=0.45536 val_ap=0.92472 time=0.31431\nEpoch  87: train_loss=0.45472 val_ap=0.92478 time=0.33230\nEpoch  88: train_loss=0.45442 val_ap=0.92438 time=0.32447\nEpoch  89: train_loss=0.45441 val_ap=0.92397 time=0.32712\nEpoch  90: train_loss=0.45332 val_ap=0.92449 time=0.31563\nEpoch  91: train_loss=0.45337 val_ap=0.92529 time=0.33210\nEpoch  92: train_loss=0.45278 val_ap=0.92598 time=0.33003\nEpoch  93: train_loss=0.45268 val_ap=0.92499 time=0.31424\nEpoch  94: train_loss=0.45226 val_ap=0.92397 time=0.32810\nEpoch  95: train_loss=0.45178 val_ap=0.92469 time=0.31845\nEpoch  96: train_loss=0.45157 val_ap=0.92645 time=0.31040\nEpoch  97: train_loss=0.45208 val_ap=0.92580 time=0.31899\nEpoch  98: train_loss=0.45138 val_ap=0.92351 time=0.31197\nEpoch  99: train_loss=0.45112 val_ap=0.92367 time=0.31345\nEpoch 100: train_loss=0.45079 val_ap=0.92559 time=0.30839\nEpoch 101: train_loss=0.45003 val_ap=0.92650 time=0.31176\nEpoch 102: train_loss=0.45016 val_ap=0.92483 time=0.30964\nEpoch 103: train_loss=0.45021 val_ap=0.92415 time=0.31566\nEpoch 104: train_loss=0.44973 val_ap=0.92547 time=0.31596\nEpoch 105: train_loss=0.44919 val_ap=0.92629 time=0.32331\nEpoch 106: train_loss=0.44914 val_ap=0.92522 time=0.32265\nEpoch 107: train_loss=0.44877 val_ap=0.92562 time=0.31672\nEpoch 108: train_loss=0.44887 val_ap=0.92712 time=0.37967\nEpoch 109: train_loss=0.44832 val_ap=0.92688 time=0.45799\nEpoch 110: train_loss=0.44830 val_ap=0.92582 time=0.40332\nEpoch 111: train_loss=0.44746 val_ap=0.92634 time=0.40980\nEpoch 112: train_loss=0.44765 val_ap=0.92800 time=0.39127\nEpoch 113: train_loss=0.44731 val_ap=0.92780 time=0.35810\nEpoch 114: train_loss=0.44710 val_ap=0.92688 time=0.37571\nEpoch 115: train_loss=0.44714 val_ap=0.92889 time=0.31958\nEpoch 116: train_loss=0.44642 val_ap=0.92805 time=0.32691\nEpoch 117: train_loss=0.44639 val_ap=0.92735 time=0.33103\nEpoch 118: train_loss=0.44628 val_ap=0.92838 time=0.34155\nEpoch 119: train_loss=0.44619 val_ap=0.92883 time=0.34280\nEpoch 120: train_loss=0.44656 val_ap=0.92850 time=0.32995\nEpoch 121: train_loss=0.44541 val_ap=0.92868 time=0.31933\nEpoch 122: train_loss=0.44558 val_ap=0.92876 time=0.32428\nEpoch 123: train_loss=0.44518 val_ap=0.92785 time=0.32366\nEpoch 124: train_loss=0.44551 val_ap=0.92873 time=0.36295\nEpoch 125: train_loss=0.44489 val_ap=0.93049 time=0.31720\nEpoch 126: train_loss=0.44509 val_ap=0.92909 time=0.31547\nEpoch 127: train_loss=0.44520 val_ap=0.92755 time=0.31548\nEpoch 128: train_loss=0.44488 val_ap=0.92904 time=0.32210\nEpoch 129: train_loss=0.44501 val_ap=0.93131 time=0.31515\nEpoch 130: train_loss=0.44428 val_ap=0.92949 time=0.31451\nEpoch 131: train_loss=0.44435 val_ap=0.92812 time=0.31936\nEpoch 132: train_loss=0.44398 val_ap=0.92963 time=0.33459\nEpoch 133: train_loss=0.44369 val_ap=0.93037 time=0.31490\nEpoch 134: train_loss=0.44351 val_ap=0.92993 time=0.32410\nEpoch 135: train_loss=0.44365 val_ap=0.92991 time=0.34138\nEpoch 136: train_loss=0.44351 val_ap=0.93010 time=0.31953\nEpoch 137: train_loss=0.44342 val_ap=0.93041 time=0.34326\nEpoch 138: train_loss=0.44291 val_ap=0.93030 time=0.32580\nEpoch 139: train_loss=0.44339 val_ap=0.93021 time=0.31841\nEpoch 140: train_loss=0.44269 val_ap=0.93172 time=0.33111\nEpoch 141: train_loss=0.44281 val_ap=0.93212 time=0.36219\nEpoch 142: train_loss=0.44255 val_ap=0.93177 time=0.32268\nEpoch 143: train_loss=0.44279 val_ap=0.93151 time=0.34769\nEpoch 144: train_loss=0.44261 val_ap=0.93176 time=0.32154\nEpoch 145: train_loss=0.44233 val_ap=0.93148 time=0.32508\nEpoch 146: train_loss=0.44247 val_ap=0.93037 time=0.36102\nEpoch 147: train_loss=0.44245 val_ap=0.93085 time=0.31309\nEpoch 148: train_loss=0.44209 val_ap=0.93273 time=0.31157\nEpoch 149: train_loss=0.44209 val_ap=0.93251 time=0.33104\nEpoch 150: train_loss=0.44188 val_ap=0.93106 time=0.32228\nEpoch 151: train_loss=0.44168 val_ap=0.93085 time=0.31400\nEpoch 152: train_loss=0.44171 val_ap=0.93280 time=0.31427\nEpoch 153: train_loss=0.44126 val_ap=0.93232 time=0.31121\nEpoch 154: train_loss=0.44174 val_ap=0.93040 time=0.33023\nEpoch 155: train_loss=0.44113 val_ap=0.93086 time=0.35015\nEpoch 156: train_loss=0.44123 val_ap=0.93203 time=0.31622\nEpoch 157: train_loss=0.44118 val_ap=0.93128 time=0.31706\nEpoch 158: train_loss=0.44080 val_ap=0.93058 time=0.33001\nEpoch 159: train_loss=0.44075 val_ap=0.93019 time=0.31961\nEpoch 160: train_loss=0.44043 val_ap=0.93076 time=0.36383\nEpoch 161: train_loss=0.44055 val_ap=0.93048 time=0.37702\nEpoch 162: train_loss=0.44022 val_ap=0.92988 time=0.33683\nEpoch 163: train_loss=0.44020 val_ap=0.92879 time=0.41915\nEpoch 164: train_loss=0.44010 val_ap=0.92874 time=0.39430\nEpoch 165: train_loss=0.43993 val_ap=0.93064 time=0.36956\nEpoch 166: train_loss=0.43975 val_ap=0.93031 time=0.39109\nEpoch 167: train_loss=0.43969 val_ap=0.92860 time=0.38634\nEpoch 168: train_loss=0.43987 val_ap=0.92926 time=0.38880\nEpoch 169: train_loss=0.43932 val_ap=0.93054 time=0.46398\nEpoch 170: train_loss=0.43929 val_ap=0.92955 time=0.35342\nEpoch 171: train_loss=0.43907 val_ap=0.93130 time=0.34771\nEpoch 172: train_loss=0.43882 val_ap=0.93178 time=0.32737\nEpoch 173: train_loss=0.43877 val_ap=0.93014 time=0.44355\nEpoch 174: train_loss=0.43862 val_ap=0.93035 time=0.50716\nEpoch 175: train_loss=0.43845 val_ap=0.93252 time=0.48247\nEpoch 176: train_loss=0.43830 val_ap=0.93403 time=0.48239\nEpoch 177: train_loss=0.43836 val_ap=0.93236 time=0.38422\nEpoch 178: train_loss=0.43794 val_ap=0.93295 time=0.37731\nEpoch 179: train_loss=0.43814 val_ap=0.93453 time=0.39103\nEpoch 180: train_loss=0.43792 val_ap=0.93471 time=0.37863\nEpoch 181: train_loss=0.43779 val_ap=0.93411 time=0.42761\nEpoch 182: train_loss=0.43784 val_ap=0.93512 time=0.34023\nEpoch 183: train_loss=0.43737 val_ap=0.93480 time=0.35506\nEpoch 184: train_loss=0.43755 val_ap=0.93493 time=0.32345\nEpoch 185: train_loss=0.43757 val_ap=0.93470 time=0.32082\nEpoch 186: train_loss=0.43716 val_ap=0.93615 time=0.34414\nEpoch 187: train_loss=0.43744 val_ap=0.93656 time=0.35454\nEpoch 188: train_loss=0.43699 val_ap=0.93532 time=0.36843\nEpoch 189: train_loss=0.43729 val_ap=0.93481 time=0.42636\nEpoch 190: train_loss=0.43659 val_ap=0.93754 time=0.36497\nEpoch 191: train_loss=0.43689 val_ap=0.93758 time=0.34891\nEpoch 192: train_loss=0.43663 val_ap=0.93570 time=0.38181\nEpoch 193: train_loss=0.43633 val_ap=0.93635 time=0.33238\nEpoch 194: train_loss=0.43619 val_ap=0.93700 time=0.31568\nEpoch 195: train_loss=0.43611 val_ap=0.93675 time=0.32621\nEpoch 196: train_loss=0.43607 val_ap=0.93651 time=0.33827\nEpoch 197: train_loss=0.43589 val_ap=0.93662 time=0.31193\nEpoch 198: train_loss=0.43549 val_ap=0.93707 time=0.34323\nEpoch 199: train_loss=0.43533 val_ap=0.93677 time=0.31987\nEpoch 200: train_loss=0.43550 val_ap=0.93673 time=0.32545\n"
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "model = GVAE(feat_dim, args['h1_dim'], args['h2_dim'], args['dropout'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "hidden_emb = None\n",
    "for epoch in range(args['epochs']):\n",
    "  t = time.time()\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  recovered, mu, logvar = model(features, adj_norm)\n",
    "  loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                       mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                       norm=norm, pos_weight=pos_weight)\n",
    "  loss.backward()\n",
    "  cur_loss = loss.item()\n",
    "  optimizer.step()\n",
    "\n",
    "  hidden_emb = mu.data.numpy()\n",
    "  roc_curr, ap_curr = gae.utils.get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "\n",
    "  print(f\"Epoch{(epoch+1):4}:\", f\"train_loss={cur_loss:.5f}\",\n",
    "        f\"val_ap={ap_curr:.5f}\", f\"time={(time.time()-t):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test ROC score: 0.9089544123948166\nTest AP score: 0.919008859720803\n"
    }
   ],
   "source": [
    "## validate\n",
    "\n",
    "roc_score, ap_score = gae.utils.get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "print('Test ROC score: ' + str(roc_score))\n",
    "print('Test AP score: ' + str(ap_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This portion runs the GCN on cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setups\n",
    "\n",
    "from gcn.models import GCN\n",
    "import gcn.utils\n",
    "\n",
    "args = {\n",
    "  'dataset': 'cora',\n",
    "  'epochs': 200,\n",
    "  'hidden_dim': 16,\n",
    "  'lr': 1e-2,\n",
    "  'weight_decay': 5e-4,\n",
    "  'dropout': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading cora dataset...\n"
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = gcn.utils.load_data()\n",
    "n_nodes, feat_dim = features.shape\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=feat_dim,\n",
    "            nhid=args['hidden_dim'],\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args['dropout'])\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args['lr'],\n",
    "                       weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 0001 loss_train: 0.1906 acc_train: 0.9786 loss_val: 0.6675 acc_val: 0.7833 time: 0.0087s\nEpoch: 0002 loss_train: 0.1842 acc_train: 0.9857 loss_val: 0.7401 acc_val: 0.7667 time: 0.0086s\nEpoch: 0003 loss_train: 0.2086 acc_train: 0.9786 loss_val: 0.7329 acc_val: 0.7900 time: 0.0078s\nEpoch: 0004 loss_train: 0.1975 acc_train: 0.9786 loss_val: 0.7111 acc_val: 0.7800 time: 0.0085s\nEpoch: 0005 loss_train: 0.1859 acc_train: 0.9714 loss_val: 0.6628 acc_val: 0.7967 time: 0.0079s\nEpoch: 0006 loss_train: 0.1984 acc_train: 0.9929 loss_val: 0.6811 acc_val: 0.8000 time: 0.0081s\nEpoch: 0007 loss_train: 0.1917 acc_train: 0.9857 loss_val: 0.6990 acc_val: 0.7600 time: 0.0076s\nEpoch: 0008 loss_train: 0.2093 acc_train: 0.9786 loss_val: 0.6491 acc_val: 0.7900 time: 0.0083s\nEpoch: 0009 loss_train: 0.1801 acc_train: 0.9857 loss_val: 0.6979 acc_val: 0.7767 time: 0.0075s\nEpoch: 0010 loss_train: 0.1952 acc_train: 0.9857 loss_val: 0.7017 acc_val: 0.7767 time: 0.0082s\nEpoch: 0011 loss_train: 0.1998 acc_train: 1.0000 loss_val: 0.7282 acc_val: 0.7867 time: 0.0082s\nEpoch: 0012 loss_train: 0.2119 acc_train: 0.9857 loss_val: 0.6995 acc_val: 0.7933 time: 0.0075s\nEpoch: 0013 loss_train: 0.2419 acc_train: 0.9929 loss_val: 0.7199 acc_val: 0.8033 time: 0.0084s\nEpoch: 0014 loss_train: 0.1747 acc_train: 0.9929 loss_val: 0.7610 acc_val: 0.7767 time: 0.0090s\nEpoch: 0015 loss_train: 0.1922 acc_train: 0.9929 loss_val: 0.7290 acc_val: 0.8033 time: 0.0082s\nEpoch: 0016 loss_train: 0.1900 acc_train: 0.9857 loss_val: 0.7507 acc_val: 0.7700 time: 0.0091s\nEpoch: 0017 loss_train: 0.1889 acc_train: 0.9786 loss_val: 0.7334 acc_val: 0.7933 time: 0.0081s\nEpoch: 0018 loss_train: 0.1931 acc_train: 0.9857 loss_val: 0.6742 acc_val: 0.7800 time: 0.0090s\nEpoch: 0019 loss_train: 0.2032 acc_train: 0.9786 loss_val: 0.6976 acc_val: 0.7967 time: 0.0079s\nEpoch: 0020 loss_train: 0.1970 acc_train: 0.9857 loss_val: 0.7036 acc_val: 0.7967 time: 0.0081s\nEpoch: 0021 loss_train: 0.2275 acc_train: 0.9714 loss_val: 0.6960 acc_val: 0.7800 time: 0.0081s\nEpoch: 0022 loss_train: 0.2267 acc_train: 0.9714 loss_val: 0.6989 acc_val: 0.7933 time: 0.0083s\nEpoch: 0023 loss_train: 0.2010 acc_train: 0.9786 loss_val: 0.6880 acc_val: 0.7833 time: 0.0073s\nEpoch: 0024 loss_train: 0.2039 acc_train: 0.9643 loss_val: 0.7216 acc_val: 0.7767 time: 0.0082s\nEpoch: 0025 loss_train: 0.2069 acc_train: 0.9643 loss_val: 0.6674 acc_val: 0.7833 time: 0.0076s\nEpoch: 0026 loss_train: 0.2301 acc_train: 0.9714 loss_val: 0.6822 acc_val: 0.7900 time: 0.0087s\nEpoch: 0027 loss_train: 0.1729 acc_train: 0.9857 loss_val: 0.7158 acc_val: 0.7933 time: 0.0075s\nEpoch: 0028 loss_train: 0.1897 acc_train: 0.9714 loss_val: 0.6931 acc_val: 0.7800 time: 0.0082s\nEpoch: 0029 loss_train: 0.1964 acc_train: 0.9857 loss_val: 0.6336 acc_val: 0.8133 time: 0.0087s\nEpoch: 0030 loss_train: 0.2377 acc_train: 0.9786 loss_val: 0.7170 acc_val: 0.7933 time: 0.0086s\nEpoch: 0031 loss_train: 0.1912 acc_train: 0.9786 loss_val: 0.6893 acc_val: 0.8000 time: 0.0078s\nEpoch: 0032 loss_train: 0.1913 acc_train: 0.9786 loss_val: 0.7319 acc_val: 0.7667 time: 0.0089s\nEpoch: 0033 loss_train: 0.2169 acc_train: 0.9643 loss_val: 0.7044 acc_val: 0.7567 time: 0.0079s\nEpoch: 0034 loss_train: 0.1651 acc_train: 0.9929 loss_val: 0.7308 acc_val: 0.7800 time: 0.0086s\nEpoch: 0035 loss_train: 0.2260 acc_train: 0.9786 loss_val: 0.7422 acc_val: 0.7833 time: 0.0078s\nEpoch: 0036 loss_train: 0.1686 acc_train: 0.9786 loss_val: 0.6939 acc_val: 0.7733 time: 0.0088s\nEpoch: 0037 loss_train: 0.2266 acc_train: 0.9571 loss_val: 0.7918 acc_val: 0.7700 time: 0.0083s\nEpoch: 0038 loss_train: 0.2181 acc_train: 0.9429 loss_val: 0.6998 acc_val: 0.7833 time: 0.0093s\nEpoch: 0039 loss_train: 0.2210 acc_train: 0.9714 loss_val: 0.7206 acc_val: 0.7733 time: 0.0087s\nEpoch: 0040 loss_train: 0.1972 acc_train: 0.9643 loss_val: 0.6772 acc_val: 0.8000 time: 0.0078s\nEpoch: 0041 loss_train: 0.2143 acc_train: 0.9714 loss_val: 0.7320 acc_val: 0.7800 time: 0.0078s\nEpoch: 0042 loss_train: 0.2174 acc_train: 0.9929 loss_val: 0.7009 acc_val: 0.7700 time: 0.0085s\nEpoch: 0043 loss_train: 0.2016 acc_train: 0.9786 loss_val: 0.7246 acc_val: 0.7867 time: 0.0079s\nEpoch: 0044 loss_train: 0.1888 acc_train: 0.9786 loss_val: 0.6867 acc_val: 0.8067 time: 0.0082s\nEpoch: 0045 loss_train: 0.1999 acc_train: 0.9714 loss_val: 0.7124 acc_val: 0.7900 time: 0.0074s\nEpoch: 0046 loss_train: 0.2088 acc_train: 0.9857 loss_val: 0.7344 acc_val: 0.7633 time: 0.0082s\nEpoch: 0047 loss_train: 0.1877 acc_train: 0.9857 loss_val: 0.6618 acc_val: 0.7900 time: 0.0072s\nEpoch: 0048 loss_train: 0.2119 acc_train: 0.9714 loss_val: 0.6431 acc_val: 0.7900 time: 0.0087s\nEpoch: 0049 loss_train: 0.1893 acc_train: 0.9786 loss_val: 0.6722 acc_val: 0.7767 time: 0.0074s\nEpoch: 0050 loss_train: 0.2079 acc_train: 0.9929 loss_val: 0.6819 acc_val: 0.7900 time: 0.0083s\nEpoch: 0051 loss_train: 0.2116 acc_train: 0.9571 loss_val: 0.6742 acc_val: 0.7833 time: 0.0082s\nEpoch: 0052 loss_train: 0.2132 acc_train: 0.9857 loss_val: 0.6735 acc_val: 0.8033 time: 0.0092s\nEpoch: 0053 loss_train: 0.1969 acc_train: 0.9786 loss_val: 0.6809 acc_val: 0.7933 time: 0.0086s\nEpoch: 0054 loss_train: 0.1971 acc_train: 0.9714 loss_val: 0.7015 acc_val: 0.7733 time: 0.0081s\nEpoch: 0055 loss_train: 0.1720 acc_train: 0.9929 loss_val: 0.7439 acc_val: 0.7733 time: 0.0076s\nEpoch: 0056 loss_train: 0.2178 acc_train: 0.9857 loss_val: 0.7185 acc_val: 0.7767 time: 0.0081s\nEpoch: 0057 loss_train: 0.1923 acc_train: 0.9714 loss_val: 0.6702 acc_val: 0.7967 time: 0.0079s\nEpoch: 0058 loss_train: 0.1836 acc_train: 0.9786 loss_val: 0.6963 acc_val: 0.7767 time: 0.0088s\nEpoch: 0059 loss_train: 0.2030 acc_train: 0.9643 loss_val: 0.7052 acc_val: 0.7667 time: 0.0079s\nEpoch: 0060 loss_train: 0.1977 acc_train: 0.9857 loss_val: 0.7093 acc_val: 0.7933 time: 0.0084s\nEpoch: 0061 loss_train: 0.1928 acc_train: 0.9929 loss_val: 0.7264 acc_val: 0.7633 time: 0.0085s\nEpoch: 0062 loss_train: 0.1976 acc_train: 0.9714 loss_val: 0.7278 acc_val: 0.7533 time: 0.0088s\nEpoch: 0063 loss_train: 0.1886 acc_train: 0.9643 loss_val: 0.7138 acc_val: 0.7633 time: 0.0085s\nEpoch: 0064 loss_train: 0.2272 acc_train: 0.9857 loss_val: 0.7181 acc_val: 0.7900 time: 0.0085s\nEpoch: 0065 loss_train: 0.2368 acc_train: 0.9714 loss_val: 0.7232 acc_val: 0.7833 time: 0.0090s\nEpoch: 0066 loss_train: 0.1943 acc_train: 0.9643 loss_val: 0.6907 acc_val: 0.7767 time: 0.0086s\nEpoch: 0067 loss_train: 0.2095 acc_train: 0.9929 loss_val: 0.6847 acc_val: 0.8300 time: 0.0090s\nEpoch: 0068 loss_train: 0.2380 acc_train: 0.9714 loss_val: 0.7192 acc_val: 0.7800 time: 0.0076s\nEpoch: 0069 loss_train: 0.2270 acc_train: 0.9714 loss_val: 0.6975 acc_val: 0.7700 time: 0.0083s\nEpoch: 0070 loss_train: 0.1819 acc_train: 0.9857 loss_val: 0.7429 acc_val: 0.7500 time: 0.0079s\nEpoch: 0071 loss_train: 0.1813 acc_train: 0.9786 loss_val: 0.6852 acc_val: 0.8033 time: 0.0081s\nEpoch: 0072 loss_train: 0.1769 acc_train: 0.9929 loss_val: 0.6609 acc_val: 0.8033 time: 0.0070s\nEpoch: 0073 loss_train: 0.2334 acc_train: 0.9714 loss_val: 0.6694 acc_val: 0.7867 time: 0.0081s\nEpoch: 0074 loss_train: 0.2013 acc_train: 0.9857 loss_val: 0.6830 acc_val: 0.8033 time: 0.0082s\nEpoch: 0075 loss_train: 0.2195 acc_train: 0.9786 loss_val: 0.7095 acc_val: 0.7967 time: 0.0088s\nEpoch: 0076 loss_train: 0.2142 acc_train: 0.9786 loss_val: 0.7419 acc_val: 0.7800 time: 0.0086s\nEpoch: 0077 loss_train: 0.2121 acc_train: 0.9786 loss_val: 0.7097 acc_val: 0.7800 time: 0.0079s\nEpoch: 0078 loss_train: 0.1958 acc_train: 0.9786 loss_val: 0.6988 acc_val: 0.7800 time: 0.0077s\nEpoch: 0079 loss_train: 0.1820 acc_train: 0.9857 loss_val: 0.6840 acc_val: 0.8000 time: 0.0080s\nEpoch: 0080 loss_train: 0.2256 acc_train: 0.9714 loss_val: 0.7419 acc_val: 0.7867 time: 0.0077s\nEpoch: 0081 loss_train: 0.2133 acc_train: 0.9786 loss_val: 0.7519 acc_val: 0.7733 time: 0.0079s\nEpoch: 0082 loss_train: 0.2018 acc_train: 0.9714 loss_val: 0.6958 acc_val: 0.7767 time: 0.0079s\nEpoch: 0083 loss_train: 0.1885 acc_train: 0.9929 loss_val: 0.6992 acc_val: 0.8033 time: 0.0078s\nEpoch: 0084 loss_train: 0.1951 acc_train: 0.9857 loss_val: 0.7667 acc_val: 0.7467 time: 0.0076s\nEpoch: 0085 loss_train: 0.1973 acc_train: 0.9643 loss_val: 0.7036 acc_val: 0.7867 time: 0.0081s\nEpoch: 0086 loss_train: 0.2168 acc_train: 0.9714 loss_val: 0.6998 acc_val: 0.7567 time: 0.0079s\nEpoch: 0087 loss_train: 0.2186 acc_train: 0.9786 loss_val: 0.6566 acc_val: 0.7967 time: 0.0080s\nEpoch: 0088 loss_train: 0.1734 acc_train: 0.9929 loss_val: 0.7153 acc_val: 0.7767 time: 0.0080s\nEpoch: 0089 loss_train: 0.2199 acc_train: 0.9857 loss_val: 0.6859 acc_val: 0.7700 time: 0.0081s\nEpoch: 0090 loss_train: 0.2116 acc_train: 0.9429 loss_val: 0.7051 acc_val: 0.7633 time: 0.0079s\nEpoch: 0091 loss_train: 0.1930 acc_train: 0.9714 loss_val: 0.7403 acc_val: 0.7600 time: 0.0079s\nEpoch: 0092 loss_train: 0.1942 acc_train: 0.9714 loss_val: 0.6767 acc_val: 0.8000 time: 0.0079s\nEpoch: 0093 loss_train: 0.1981 acc_train: 0.9714 loss_val: 0.7116 acc_val: 0.7800 time: 0.0075s\nEpoch: 0094 loss_train: 0.1793 acc_train: 0.9857 loss_val: 0.7336 acc_val: 0.7967 time: 0.0083s\nEpoch: 0095 loss_train: 0.2109 acc_train: 0.9857 loss_val: 0.6946 acc_val: 0.7800 time: 0.0075s\nEpoch: 0096 loss_train: 0.2164 acc_train: 0.9714 loss_val: 0.6981 acc_val: 0.7900 time: 0.0083s\nEpoch: 0097 loss_train: 0.1728 acc_train: 0.9857 loss_val: 0.6804 acc_val: 0.7767 time: 0.0076s\nEpoch: 0098 loss_train: 0.1987 acc_train: 0.9929 loss_val: 0.7064 acc_val: 0.7933 time: 0.0083s\nEpoch: 0099 loss_train: 0.2056 acc_train: 0.9714 loss_val: 0.7216 acc_val: 0.7867 time: 0.0078s\nEpoch: 0100 loss_train: 0.1874 acc_train: 1.0000 loss_val: 0.7039 acc_val: 0.7867 time: 0.0093s\nEpoch: 0101 loss_train: 0.1919 acc_train: 0.9714 loss_val: 0.6889 acc_val: 0.7833 time: 0.0074s\nEpoch: 0102 loss_train: 0.2181 acc_train: 0.9857 loss_val: 0.7183 acc_val: 0.7667 time: 0.0085s\nEpoch: 0103 loss_train: 0.2044 acc_train: 0.9714 loss_val: 0.6891 acc_val: 0.7833 time: 0.0080s\nEpoch: 0104 loss_train: 0.2258 acc_train: 0.9714 loss_val: 0.6973 acc_val: 0.8067 time: 0.0082s\nEpoch: 0105 loss_train: 0.1875 acc_train: 0.9786 loss_val: 0.6915 acc_val: 0.7967 time: 0.0076s\nEpoch: 0106 loss_train: 0.1963 acc_train: 0.9929 loss_val: 0.6821 acc_val: 0.8067 time: 0.0081s\nEpoch: 0107 loss_train: 0.1898 acc_train: 0.9857 loss_val: 0.7117 acc_val: 0.7967 time: 0.0078s\nEpoch: 0108 loss_train: 0.1878 acc_train: 0.9857 loss_val: 0.7030 acc_val: 0.7833 time: 0.0082s\nEpoch: 0109 loss_train: 0.2086 acc_train: 0.9714 loss_val: 0.7587 acc_val: 0.7600 time: 0.0074s\nEpoch: 0110 loss_train: 0.2103 acc_train: 0.9929 loss_val: 0.7065 acc_val: 0.7833 time: 0.0080s\nEpoch: 0111 loss_train: 0.1972 acc_train: 0.9929 loss_val: 0.7249 acc_val: 0.7733 time: 0.0075s\nEpoch: 0112 loss_train: 0.2279 acc_train: 0.9857 loss_val: 0.7275 acc_val: 0.7700 time: 0.0080s\nEpoch: 0113 loss_train: 0.2105 acc_train: 0.9571 loss_val: 0.7107 acc_val: 0.8067 time: 0.0076s\nEpoch: 0114 loss_train: 0.2080 acc_train: 0.9857 loss_val: 0.7224 acc_val: 0.8067 time: 0.0079s\nEpoch: 0115 loss_train: 0.1998 acc_train: 0.9714 loss_val: 0.7186 acc_val: 0.7867 time: 0.0074s\nEpoch: 0116 loss_train: 0.2075 acc_train: 0.9714 loss_val: 0.7186 acc_val: 0.7800 time: 0.0081s\nEpoch: 0117 loss_train: 0.2055 acc_train: 0.9786 loss_val: 0.7314 acc_val: 0.7800 time: 0.0076s\nEpoch: 0118 loss_train: 0.1949 acc_train: 0.9857 loss_val: 0.6839 acc_val: 0.7933 time: 0.0079s\nEpoch: 0119 loss_train: 0.1952 acc_train: 0.9786 loss_val: 0.7263 acc_val: 0.7867 time: 0.0078s\nEpoch: 0120 loss_train: 0.2220 acc_train: 0.9786 loss_val: 0.7151 acc_val: 0.7733 time: 0.0080s\nEpoch: 0121 loss_train: 0.2027 acc_train: 0.9857 loss_val: 0.7308 acc_val: 0.8033 time: 0.0079s\nEpoch: 0122 loss_train: 0.2244 acc_train: 0.9857 loss_val: 0.7213 acc_val: 0.7633 time: 0.0080s\nEpoch: 0123 loss_train: 0.2219 acc_train: 0.9714 loss_val: 0.6666 acc_val: 0.7933 time: 0.0079s\nEpoch: 0124 loss_train: 0.2075 acc_train: 0.9714 loss_val: 0.6635 acc_val: 0.7933 time: 0.0083s\nEpoch: 0125 loss_train: 0.1972 acc_train: 0.9857 loss_val: 0.7251 acc_val: 0.7933 time: 0.0087s\nEpoch: 0126 loss_train: 0.2006 acc_train: 0.9857 loss_val: 0.6535 acc_val: 0.7867 time: 0.0083s\nEpoch: 0127 loss_train: 0.2213 acc_train: 0.9786 loss_val: 0.6771 acc_val: 0.7833 time: 0.0081s\nEpoch: 0128 loss_train: 0.1933 acc_train: 0.9786 loss_val: 0.7754 acc_val: 0.7767 time: 0.0082s\nEpoch: 0129 loss_train: 0.2053 acc_train: 0.9857 loss_val: 0.7305 acc_val: 0.7933 time: 0.0080s\nEpoch: 0130 loss_train: 0.2058 acc_train: 0.9929 loss_val: 0.7179 acc_val: 0.7700 time: 0.0078s\nEpoch: 0131 loss_train: 0.1844 acc_train: 0.9857 loss_val: 0.7074 acc_val: 0.7833 time: 0.0077s\nEpoch: 0132 loss_train: 0.2063 acc_train: 0.9643 loss_val: 0.6978 acc_val: 0.7800 time: 0.0083s\nEpoch: 0133 loss_train: 0.2081 acc_train: 0.9857 loss_val: 0.7299 acc_val: 0.7667 time: 0.0083s\nEpoch: 0134 loss_train: 0.2199 acc_train: 0.9857 loss_val: 0.7045 acc_val: 0.7633 time: 0.0079s\nEpoch: 0135 loss_train: 0.1956 acc_train: 0.9929 loss_val: 0.7087 acc_val: 0.7900 time: 0.0083s\nEpoch: 0136 loss_train: 0.2080 acc_train: 0.9714 loss_val: 0.7140 acc_val: 0.8000 time: 0.0075s\nEpoch: 0137 loss_train: 0.1861 acc_train: 0.9857 loss_val: 0.7302 acc_val: 0.7833 time: 0.0076s\nEpoch: 0138 loss_train: 0.2225 acc_train: 0.9714 loss_val: 0.6974 acc_val: 0.7667 time: 0.0076s\nEpoch: 0139 loss_train: 0.2157 acc_train: 0.9786 loss_val: 0.7002 acc_val: 0.7700 time: 0.0079s\nEpoch: 0140 loss_train: 0.1968 acc_train: 0.9857 loss_val: 0.6742 acc_val: 0.8100 time: 0.0086s\nEpoch: 0141 loss_train: 0.1959 acc_train: 0.9857 loss_val: 0.7309 acc_val: 0.8000 time: 0.0079s\nEpoch: 0142 loss_train: 0.1747 acc_train: 0.9929 loss_val: 0.6912 acc_val: 0.7900 time: 0.0082s\nEpoch: 0143 loss_train: 0.1917 acc_train: 0.9929 loss_val: 0.7441 acc_val: 0.7700 time: 0.0077s\nEpoch: 0144 loss_train: 0.1864 acc_train: 0.9857 loss_val: 0.6868 acc_val: 0.7800 time: 0.0074s\nEpoch: 0145 loss_train: 0.1851 acc_train: 0.9857 loss_val: 0.7468 acc_val: 0.7667 time: 0.0081s\nEpoch: 0146 loss_train: 0.2080 acc_train: 0.9714 loss_val: 0.7648 acc_val: 0.7600 time: 0.0073s\nEpoch: 0147 loss_train: 0.1930 acc_train: 0.9857 loss_val: 0.7408 acc_val: 0.7600 time: 0.0082s\nEpoch: 0148 loss_train: 0.1927 acc_train: 0.9857 loss_val: 0.7359 acc_val: 0.7733 time: 0.0074s\nEpoch: 0149 loss_train: 0.2039 acc_train: 0.9857 loss_val: 0.7148 acc_val: 0.7800 time: 0.0093s\nEpoch: 0150 loss_train: 0.1927 acc_train: 0.9857 loss_val: 0.7526 acc_val: 0.7567 time: 0.0075s\nEpoch: 0151 loss_train: 0.1862 acc_train: 1.0000 loss_val: 0.7239 acc_val: 0.7867 time: 0.0082s\nEpoch: 0152 loss_train: 0.2144 acc_train: 0.9786 loss_val: 0.6767 acc_val: 0.7833 time: 0.0080s\nEpoch: 0153 loss_train: 0.1812 acc_train: 0.9857 loss_val: 0.7328 acc_val: 0.7867 time: 0.0092s\nEpoch: 0154 loss_train: 0.2259 acc_train: 0.9786 loss_val: 0.6867 acc_val: 0.8100 time: 0.0079s\nEpoch: 0155 loss_train: 0.2190 acc_train: 0.9786 loss_val: 0.6801 acc_val: 0.7900 time: 0.0080s\nEpoch: 0156 loss_train: 0.1862 acc_train: 0.9929 loss_val: 0.6971 acc_val: 0.8000 time: 0.0080s\nEpoch: 0157 loss_train: 0.2209 acc_train: 0.9786 loss_val: 0.6439 acc_val: 0.8167 time: 0.0082s\nEpoch: 0158 loss_train: 0.2094 acc_train: 0.9714 loss_val: 0.7408 acc_val: 0.7800 time: 0.0074s\nEpoch: 0159 loss_train: 0.1936 acc_train: 0.9857 loss_val: 0.6974 acc_val: 0.8033 time: 0.0084s\nEpoch: 0160 loss_train: 0.1874 acc_train: 0.9857 loss_val: 0.6905 acc_val: 0.7767 time: 0.0076s\nEpoch: 0161 loss_train: 0.2079 acc_train: 0.9714 loss_val: 0.6838 acc_val: 0.8067 time: 0.0078s\nEpoch: 0162 loss_train: 0.2081 acc_train: 0.9714 loss_val: 0.6956 acc_val: 0.7800 time: 0.0077s\nEpoch: 0163 loss_train: 0.1997 acc_train: 0.9786 loss_val: 0.6794 acc_val: 0.8100 time: 0.0084s\nEpoch: 0164 loss_train: 0.2040 acc_train: 0.9714 loss_val: 0.7283 acc_val: 0.7700 time: 0.0077s\nEpoch: 0165 loss_train: 0.2062 acc_train: 0.9857 loss_val: 0.7303 acc_val: 0.8000 time: 0.0081s\nEpoch: 0166 loss_train: 0.2575 acc_train: 0.9500 loss_val: 0.7061 acc_val: 0.7600 time: 0.0072s\nEpoch: 0167 loss_train: 0.2276 acc_train: 0.9786 loss_val: 0.6955 acc_val: 0.7767 time: 0.0082s\nEpoch: 0168 loss_train: 0.1991 acc_train: 0.9786 loss_val: 0.6847 acc_val: 0.7900 time: 0.0070s\nEpoch: 0169 loss_train: 0.1708 acc_train: 0.9786 loss_val: 0.7077 acc_val: 0.7733 time: 0.0081s\nEpoch: 0170 loss_train: 0.2150 acc_train: 0.9571 loss_val: 0.6686 acc_val: 0.8133 time: 0.0075s\nEpoch: 0171 loss_train: 0.1742 acc_train: 0.9857 loss_val: 0.7325 acc_val: 0.7733 time: 0.0079s\nEpoch: 0172 loss_train: 0.2073 acc_train: 0.9786 loss_val: 0.6925 acc_val: 0.7867 time: 0.0079s\nEpoch: 0173 loss_train: 0.2466 acc_train: 0.9571 loss_val: 0.6594 acc_val: 0.8033 time: 0.0078s\nEpoch: 0174 loss_train: 0.2178 acc_train: 0.9714 loss_val: 0.7360 acc_val: 0.7733 time: 0.0080s\nEpoch: 0175 loss_train: 0.1769 acc_train: 0.9857 loss_val: 0.7026 acc_val: 0.7733 time: 0.0091s\nEpoch: 0176 loss_train: 0.1735 acc_train: 0.9929 loss_val: 0.6786 acc_val: 0.8200 time: 0.0086s\nEpoch: 0177 loss_train: 0.2118 acc_train: 0.9571 loss_val: 0.7035 acc_val: 0.7867 time: 0.0088s\nEpoch: 0178 loss_train: 0.2141 acc_train: 0.9643 loss_val: 0.7330 acc_val: 0.7533 time: 0.0077s\nEpoch: 0179 loss_train: 0.2029 acc_train: 0.9714 loss_val: 0.7017 acc_val: 0.8033 time: 0.0080s\nEpoch: 0180 loss_train: 0.1849 acc_train: 0.9857 loss_val: 0.6975 acc_val: 0.7833 time: 0.0080s\nEpoch: 0181 loss_train: 0.1661 acc_train: 0.9786 loss_val: 0.7431 acc_val: 0.7867 time: 0.0083s\nEpoch: 0182 loss_train: 0.1866 acc_train: 0.9857 loss_val: 0.7215 acc_val: 0.7800 time: 0.0080s\nEpoch: 0183 loss_train: 0.2144 acc_train: 0.9857 loss_val: 0.7250 acc_val: 0.7800 time: 0.0090s\nEpoch: 0184 loss_train: 0.1904 acc_train: 0.9857 loss_val: 0.7218 acc_val: 0.7567 time: 0.0081s\nEpoch: 0185 loss_train: 0.2102 acc_train: 0.9786 loss_val: 0.7186 acc_val: 0.7733 time: 0.0083s\nEpoch: 0186 loss_train: 0.2343 acc_train: 0.9714 loss_val: 0.6974 acc_val: 0.7867 time: 0.0071s\nEpoch: 0187 loss_train: 0.2158 acc_train: 0.9929 loss_val: 0.7530 acc_val: 0.7600 time: 0.0080s\nEpoch: 0188 loss_train: 0.2236 acc_train: 0.9571 loss_val: 0.6874 acc_val: 0.7800 time: 0.0075s\nEpoch: 0189 loss_train: 0.2217 acc_train: 0.9786 loss_val: 0.6929 acc_val: 0.7800 time: 0.0079s\nEpoch: 0190 loss_train: 0.2049 acc_train: 0.9786 loss_val: 0.7197 acc_val: 0.7700 time: 0.0077s\nEpoch: 0191 loss_train: 0.2411 acc_train: 0.9714 loss_val: 0.6981 acc_val: 0.7967 time: 0.0084s\nEpoch: 0192 loss_train: 0.1866 acc_train: 0.9857 loss_val: 0.6758 acc_val: 0.7967 time: 0.0078s\nEpoch: 0193 loss_train: 0.2016 acc_train: 0.9786 loss_val: 0.6694 acc_val: 0.7933 time: 0.0078s\nEpoch: 0194 loss_train: 0.1629 acc_train: 1.0000 loss_val: 0.7124 acc_val: 0.7733 time: 0.0077s\nEpoch: 0195 loss_train: 0.1789 acc_train: 0.9857 loss_val: 0.6969 acc_val: 0.7833 time: 0.0081s\nEpoch: 0196 loss_train: 0.2263 acc_train: 0.9786 loss_val: 0.7068 acc_val: 0.7833 time: 0.0077s\nEpoch: 0197 loss_train: 0.1796 acc_train: 0.9929 loss_val: 0.6844 acc_val: 0.7867 time: 0.0077s\nEpoch: 0198 loss_train: 0.2066 acc_train: 0.9643 loss_val: 0.7109 acc_val: 0.7833 time: 0.0078s\nEpoch: 0199 loss_train: 0.2390 acc_train: 0.9714 loss_val: 0.7364 acc_val: 0.7733 time: 0.0092s\nEpoch: 0200 loss_train: 0.1791 acc_train: 0.9857 loss_val: 0.6585 acc_val: 0.7933 time: 0.0080s\n(2708, 16)\nOptimization Finished!\nTotal time elapsed: 1.7000s\n"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "  t = time.time()\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  output = model(features, adj)\n",
    "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "  acc_train = gcn.utils.accuracy(output[idx_train], labels[idx_train])\n",
    "  loss_train.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "  acc_val = gcn.utils.accuracy(output[idx_val], labels[idx_val])\n",
    "  print(f'Epoch: {(epoch+1):04d}',\n",
    "        f'loss_train: {loss_train.item():.4f}',\n",
    "        f'acc_train: {acc_train.item():.4f}',\n",
    "        f'loss_val: {loss_val.item():.4f}',\n",
    "        f'acc_val: {acc_val.item():.4f}',\n",
    "        f'time: {(time.time() - t):.4f}s')\n",
    "\n",
    "npemb = model.hidden_emb.detach().numpy()\n",
    "print(npemb.shape)\n",
    "np.savetxt('hidden_emb.content', npemb)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(f\"Total time elapsed: {time.time() - t_total:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test set results: loss= 0.6041 accuracy= 0.8320\n"
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "model.eval()\n",
    "output = model(features, adj)\n",
    "loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "acc_test = gcn.utils.accuracy(output[idx_test], labels[idx_test])\n",
    "print(f\"Test set results:\",\n",
    "      f\"loss= {loss_test.item():.4f}\",\n",
    "      f\"accuracy= {acc_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}